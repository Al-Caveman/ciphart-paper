\documentclass[twocolumn]{article}
\usepackage[margin=.7in]{geometry}
\usepackage[showisoZ=false]{datetime2}
\usepackage{url}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{security}{security interpretation}
\newtheorem{definition}{definition}
\newtheorem{theorem}{theorem}
\newtheorem{note}{note}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\DeclareMathOperator{\enc}{\mathtt{enc}}
\DeclareMathOperator{\dec}{\mathtt{dec}}
\DeclareMathOperator{\maxf}{max}
\DeclareMathOperator{\len}{len}
\DeclareMathOperator{\hash}{\mathtt{hash}}
\DeclareMathOperator{\rhash}{\mathtt{rhash}}
\DeclareMathOperator{\mhash}{\mathtt{mhash}}
\DeclareMathOperator{\argon}{\mathtt{argon2}}
\DeclareMathOperator{\ciphart}{\mathtt{ciphart}}
\DeclareMathOperator{\cost}{\mathtt{cost}}
\DeclareMathOperator{\henc}{\; HENC}
\DeclareMathOperator{\hhash}{\; HHASH}
\renewcommand{\contentsname}{paper's layout}
\makeatletter
\def\myrulefill{%
    \leavevmode\leaders\hrule%
    height .6ex width 1ex depth -0.4ex%
    \hfill\kern\z@%
}
\makeatother
\DTMsetdatestyle{iso}
\usepackage{cleveref} % must be loaded last
\begin{document}
\SetAlgorithmName{algorithm}{}{list of algorithms}
\SetInd{.15em}{1em}

\begin{center}
\Huge
\myrulefill\ ciphart \myrulefill\\
\LARGE
memory-harder key derivation \\
with easier measurable security\\
\normalsize
caveman\footnote{mail: toraboracaveman [at] protonmail [dot] com}\\
\footnotesize
\DTMnow\\
\rule{1\columnwidth}{2pt}
\end{center}

argon2\footnote{\url{https://github.com/P-H-C/phc-winner-argon2}} is mostly
nice, as it is memory-hard while still being relatively simple to
understand its contribution to the overall security, which is an
improvement over \emph{scrypt}.  but i argue that \emph{argon2} is still
not nice enough, as they can be harder and simpler, respectively.

currently, if you want to know what \emph{argon2} is giving for you
security-wise, you need to survey the industry of application-specific
integrated-circuits (asics) in order to obtain a cost-money map, as done in
the \emph{scrypt} paper.  this has too much housekeeping as the industry is
constantly changing, plus it remains a heuristic with inadequate
theoretical guarantees.

henceforth, i propose \emph{ciphart} a memory-\emph{harder} key derivation
function, with a security contribution that is measured in the unit of
shannon's entropy.  i.e. \emph{ciphart} can say that it is injecting $x$
many shannon's entropy bits to your password.

\begin{itemize}
    \item the memory-harder\emph{ness} comes from also utilising disk
    space.  this is practically conveniently possible thanks to my
    discovery of the fact that caching keys into disks, is practical for
    almost every use cases.  

    this disk caching only needs to assume that \emph{root} is trusty,
    which is not a new assumption for most use cases.  thus, this disk
    caching does not really reduce our security, but opens doors for us to
    lead us to memory-\emph{harder} key derivation functions, which
    increases our security.

    nonetheless, everything in this bullet point is optional, just in case
    you're feeling uneasy about it.  i personally use it as it
    significantly enhances my security, and you won't compromise my
    security, even if you steal my disk.

    \item the simpler security interpretation is thanks to \emph{the
    perfect lie} theorem that i propose, which allows \emph{ciphart} to
    claim that it is injecting shannon's entropy bits into input passwords,
    for as long as the password remains a secret.
\end{itemize}

\texttt{libciphart}\footnote{\url{https://github.com/Al-Caveman/libciphart}}
is a library that implements \emph{ciphart} very closely to this paper,
without much fluff.  this should make integrating \emph{ciphart} into other
systems more convenient.

\texttt{ciphart}\footnote{\url{https://github.com/Al-Caveman/ciphart}} is
an application for encrypting and decrypting files that makes use of
\texttt{libciphart}.  this application is intended for use by end-users or
scripts, henceforth it has some fluff to treat mankind with dignity.

\tableofcontents
\noindent
\rule{1\columnwidth}{2pt}

\section{background}
we've got password $p$ with $H(p)$ many shannon's entropy bits worth of
information in it.  so what does this mean?

fundamentally, it means that, on average, we'd need to ask $H(p)$ many
perfect binary questions\footnote{one which, if answered, and on average,
gets the search space reduced in half.} in order to fully resolve all
ambiguities about $p$; i.e.  to fully get every bit of $p$.  

but people use it to do less orthodox things, such as quantifying the
amount of security $p$ has against, say, brute-forcing attacks.

say that we've got a $8V$ bit key $k \gets \hash(p \Vert s, 8V)$, derived
from password $p$, where $s$ is a salt.  say that the attacker has $s$ and
$k$ but wants to figure out $p$.  in this case, he will need to brute-force
the password space in order to find $p$ that gives $k$.  his cost is:
\begin{equation}\label{eq_cost_passbruteforce}
    2^{H(p)} \left(
        \cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}

\begin{definition}
the security of a system is the cost of the cheapest method that can break
it.
\end{definition}

one way to estimate $\cost$ is to survey the asics industry.  by surveying
the asics industry to get an idea how much money it costs to get a given
key, or password, space brute-forced within a target time
frame\footnote{see the \emph{scrypt} paper for an example.}.  this has an
expensive housekeeping and is usually not possible to get any guarantees as
we don't know about state-of-art manufacturing secrets that adversaries may
have.

another way is to ignore anything that has no cryptographic guarantee.  so,
in (\ref{eq_cost_passbruteforce}), cryptography
guarantees\footnote{statistically by confidence earned through peer review
and attempts to break encryption algorithms.} that $2^{H(p)}$ many $\hash$
calls are performed and that many equality tests.  the $\hash$ call needs
to be done once, so let's give it a unit of time $1$.  the equality test
also needs to be called once, but since since it's so cheap it's easier to
just assume that its cost is free.  this way (\ref{eq_cost_passbruteforce})
becomes just:
\begin{equation}\label{eq_simplecost_passbruteforce}
    2^{H(p)} (1+0) = 2^{H(p)}
\end{equation}

further, for convenience, it seems that people report it in the $\log_2$
scale.  i.e. $\log_2 2^{H(p)} = H(p)$.  i think this is why people use
password entropy as a measure of its security.  not because it is the
quantity of security, but rather because its the quantity of
\emph{simplified} security.  

\section{caveman's entropy}
\subsection{recursive $\hash$}
if the $\hash$ function is replaced by an $N$-deep recursion over $\hash$,
like:
\[
    \begin{split}
        & \rhash(p \Vert s, 8V, N) \\
    ={} &  \hash(\hash(\ldots\hash(p \Vert s, 8V), \ldots, 8V), 8V)
    \end{split}
\]
then, if $\hash$ is not broken,  (\ref{eq_cost_passbruteforce}) becomes:
\begin{equation}\label{eq_cost_passbruteforce_N}
    2^{H(p)} \left(
        N\cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}
and (\ref{eq_simplecost_passbruteforce}) becomes:
\begin{equation}\label{eq_simplecost_passbruteforce_N}
    \begin{split}
    2^{H(p)} (N+0) &= N2^{H(p)} \\
                  &= 2^{H(p) + \log_2 N}
    \end{split}
\end{equation}

at this point, thanks to cryptographic guarantees concerning properties of
hashing functions, there is absolutely no security distinction between a
password with shannon's $H(p) + \log_2 N$ entropy bits, and a password with
just $H(p)$ entropy bits that made use of the $N$-deep recursive calls of
$\hash$.

shannon's entropy of $p$ remains $H(p)$, but thanks to the recursive calls
of $\hash$, that password will be as expensive as another password $\hat
p$, such that $H(\hat p) = H(p) + \log_2 N$.

i think it will be simpler if we introduce the function-dependent caveman's
entropy $C$ as a measure.  it goes like this:
\begin{equation}
    C\Big(p, \hash(\ldots)\Big) = H(p)
\end{equation}

\begin{equation}
    C\Big(\hat p, \hash(\ldots)\Big) = H(p) + \log_2 N
\end{equation}

\begin{equation}
    \begin{split}
        C\Big(p, \rhash(\ldots, N)\Big) &= H(p) + \log_2 N \\
                                &= H(\hat p) \\
    \end{split}
\end{equation}

security-wise, there is no distinction between the more complex password
$\hat p$, and the simpler password $p$ that used $\rhash(\ldots, N)$.  so i
really think we need to measure password security in $C$ instead of $H$.

\subsection{memory-hard $\hash$}
let $\mhash$ be like $\rhash$, except that it also requires $M$ many memory
bytes such that, as available memory is linearly reduced from $M$, penalty
in cpu time grows exponentially.  let $M$ be requested memory, $\hat M$ be
available memory, and $e(M - \hat M)$ be the exponential penalty value for
reduction in memory, where $e(0) = 1$.
\begin{equation}
    \begin{split}
        & \cost\Big(\mhash(p \Vert s, N, M)\Big) \\
    ={} & \cost\Big(\rhash(p \Vert s, N)\Big)^{e(\hat M - M)}
    \end{split}
\end{equation}

if $\hash$ in (\ref{eq_cost_passbruteforce}) is replaced by the $M$-bytes
memory-hardened $N$-deep recursion hash function $\mhash$, then
(\ref{eq_cost_passbruteforce}) becomes:
\begin{equation}\label{eq_cost_passbruteforce_NM}
    2^{H(p)} \left(
        N^{e(M-\hat M)}\cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}
(\ref{eq_simplecost_passbruteforce}) becomes:
\begin{equation}\label{eq_simplecost_passbruteforce_NM}
    \begin{split}
    2^{H(p)} (N^{e(M-\hat M)}+0) &= N^{e(M-\hat M)} 2^{H(p)} \\
                  &= 2^{H(p) + \log_2 N^{e(M-\hat M)}} \\
                  &= 2^{H(p) + e(M-\hat M)\log_2 N}
    \end{split}
\end{equation}
and caveman's entropy becomes:
\begin{equation}
    C\Big(p, \mhash(\ldots, N, M)\Big) = H(p) + e(M-\hat M)\log_2 N
\end{equation}

\section{the perfect lie theorem}
let $p$ be a password with $H(p)$ shannon's entropy bits.  let $\hat p$ be
a more complex password with $H(p) + e(M-\hat M)\log_2 N$ shannon's entropy
bits, where $M$, $\hat M$ and $N$ are all positive numbers.

then caveman's entropy says that the following keys are information
theoretically indistinguishable for as long as only $p$ and $\hat p$ remain
unknown (everything else is known, such as the distribution from which $p$
and $\hat p$ was sampled), and for as long as $\hash$ is not broken:
\begin{itemize}
    \item $k \gets \mhash(p \Vert s, N, M)$
    \item $\hat k \gets \hash(\hat p \Vert s)$
\end{itemize}

in other words:
\begin{equation}
    C\Big(p, \mhash(\ldots, N, M)\Big) = H(\hat p)
\end{equation}

since the assumption that passwords are kept away from the adversary is
fundamental in a symmetric encryption context, i think it makes since that
we measure our security with memory-hard key derivation functions using the
caveman's entropy $C$ instead of shannon's entropy $H$.

from a security point of view, it will feel absolutely identical to as if
the password got injected with extra shannon's entropy bits.  no one can
tell the difference for as long as the fundamental assumption of hiding
passwords is honoured, as well as the hashing function $\hash$ is not
broken.

in other words, we can say, if password $p$ is unknown, and $\hash$ is not
broken, then we have injected into $p$ extra shannon's entropy bits.  this
lie will be only discovered after $p$ is revealed.

if you think that it is impossible for this \emph{lie} to be \emph{truth}
under the secrecy of $p$, then i've done an even better job: proving that
cryptographically secure hashing functions do not exist.  likewise, same
can be trivially extended to: cryptographically symmetric ciphers do not
exist.

so you have to pick only one of these options:
\begin{enumerate}
    \item either accept that the lie is truth.  i.e. accept that we've
    injected shannon's entropy bits into $p$, for as long as only $p$ is
    not revealed.
    \item or, accept that cryptographically-secure hashing and
    symmetric-encryption functions functions do not exist.
\end{enumerate}

\begin{theorem}[the perfect lie]\label{theorem_perfect_lie}
when $p$ is secret and $\hash$ is not broken, then shannon's entropy $H$ of
the derived key equals caveman's entropy $C$.
\end{theorem}

i call \cref{theorem_perfect_lie} \emph{the perfect lie} theorem in a sense
that a perfect lie is indistinguishable from truth.

the reason this lie is appealing is because it simplifies our
quantification of the amount of security that we have gained by using a
given key derivation function, such as $\rhash$ or $\mhash$.

without treating this lie as truth, our only hope would be surveying the
asics industry.  but with this lie, we have one more approach to get a feel
of the gained security quantity by just accepting caveman's entropy $C$ as
shannon's entropy $H$, and move on as if the lie is truth, and no one can
notice it.

we can also look at it from the perspective of \emph{occam's razor}.  i.e.
if two things are not distinguishable from one another, then assuming that
they are just the same thing is simpler than assuming otherwise.  

to be more specific about \emph{occam's razor}: (1) each assumption bit has
a positive probability of error by definition, (2) since assuming that
indistinguishable things are different than one another is more complex
(i.e. more assumption bits) than assuming not, and (3) since there is no
observable difference between the two things, therefore it necessarily
follows that our model's total error will be reduced if we accept that the
indistinguishable things are identical (i.e.  which is what
\cref{theorem_perfect_lie} says).

\section{ciphart}
\subsection{parameters}
\begin{tabularx}{\columnwidth}{lX}
    $p$ & password.\\
    $s$ & salt.\\
    $M$ & total memory in bytes.\\
    $L$ & number of memory lanes for concurrency.\\
    $T$ & number of tasks per lane segment.\\
    $B$ & minimum \emph{caveman's entropy bits} to inject into $p$.\\
    $K$ & output key's size in bytes.\\
\end{tabularx}

\subsection{internal variables}
\begin{tabularx}{\columnwidth}{lX}
    $\enc$ & encryption function.\\
    $\hash$ & hashing function.\\
    $C$         & $\gets \begin{cases}
                        64 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is \emph{aes}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this to reflect the block size of the encryption
                    algorithm that implements $\enc$.\\
    $V$ & $\gets \begin{cases}
                        32 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-128}}\\
                        32 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-256}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this is the size of the encryption key that's used to
                    solve \emph{ciphart}'s tasks.  this is different than
                    the $\enc$-independent $K$ which is
                    possibly used by other encryption algorithms in later
                    stages\footnote{at the expense of losing the meaning of
                    \emph{caveman's entropy bits}.}.\\
    $\hat T$    & $\gets \maxf(\lceil V C^{-1}\rceil, T)$.  this
                    is to ensure that we have enough encrypted bytes for
                    new keys.\\
    $\hat T$    & $\gets \hat T - (\hat T \bmod 2) + 2$.  this is to ensure
                    that there is an even number of tasks in a segment.
                    why?  because we need a buffer for storing the
                    clear-text and another for storing the output
                    cipher-text.\\
    $\hat M$    & $\gets M - (M \bmod C\hat TL) + C\hat TL$.  this is to
                    ensure that it is in multiples of $C\hat TL$.  why?  so
                    that all segments are of equal lengths in order to
                    simplify \emph{ciphart}'s logic.  e.g. it wouldn't be
                    nice if the last segments were of unequal sizes.\\
    $G$         & $\gets \hat MC^{-1}\hat T^{-1}L^{-1}$.  total number of
                    segments per lane.\\
    $N$    & $\gets 0$.  actual number of times $\enc$ is called,
                    where $\hat N \ge 2^B$.\\
    $m_i$       & $C$-bytes memory for $i^{th}$ task in the $\hat M$-bytes
                    pad.\\
    $n_l$       & $\gets lG\hat T$.  nonce variable for $l^{th}$ lane with
                    at least $64$ bits.\\
    $f$         & $\gets 0$.  a flag indicating whether the $\hat M$-bytes
                    pad is filled.\\
    $v$         & $\gets *\hash(p \mathbin\Vert s, V)$.  a pointer to the
                    first byte where $V$-bytes key is stored.\\
\end{tabularx}

\subsection{output}
\begin{tabularx}{\columnwidth}{lX}
$k$ & $K$-bytes key.\\
$\hat B$ & actual \emph{caveman's entropy bits} that were injected into
            $p$, where $\hat B \ge B$.\\
\end{tabularx}

\subsection{steps}
steps of \emph{ciphart} is shown in \cref{alg_ciphart}.  this corresponds
to \emph{argon2d}.  adding a \emph{ciphart-i} variant is a trivial matter,
i just didn't do it yet because my threat model currently doesn't benefit
from a password independent variant.

\begin{algorithm}[tbh]
\While{$1$}{
    \For{$g=0, 1, \ldots, G-1$}{
        \For{$l=0, 1, \ldots, L-1$}{\label{ciphart_lanes}
            \For{$t=0, 1, \ldots, T-1$}{
                $i \gets gLT + lT + t$\;
                \uIf{$t < T - 1$}{
                    $j \gets i + 1$\;
                }\ElseIf{$t = T - 1$}{
                    $j \gets i - T + 1$\;
                }
                $m_j \gets \enc(m_i, n_l, v)$\;
                $n_l \gets n_l + 1$\;
                \uIf{$f = 0$}{
                    $v \gets m_j \bmod (gLTC + tC - V)$\;
                    \If{$v \ge gLTC - V$}{
                        $v \gets v + lTC$\;
                    }
                }\Else{
                    $v \gets m_j \bmod (\hat M - LTC + tC - V)$\;
                    \uIf{$v \ge gLTC + tC - V$}{
                        $v \gets v + LTC$\;
                    }\ElseIf{$v \ge gLTC - V$}{
                        $v \gets v + lTC$\;
                    }
                }
            }
        }
        $N \gets N + LT$\;
        \If{$N \ge 2^B$}{
            $g_{\text{last}} \gets g$\;
            \textbf{go to} \cref{ciphart_out}\;
        }
    }
    $f \gets 1$\;
}
$i \gets g_{\text{last}}LT$\;\label{ciphart_out}
$k \gets \hash(m_{i+0T} \Vert m_{i+1T} \Vert \ldots \Vert m_{i+(L-1)T}, K)$\;
$\hat B \gets log_2 N$\;
\Return{$k$, $\hat B$}
\caption{ciphart}
\label{alg_ciphart}
\end{algorithm}

\section{parallelism}
since iterations of the loop in \cref{ciphart_lanes} in \cref{alg_ciphart}
are fully independent of one other, they can quite happily utilise $L$ cpu
cores, specially when segment sizes, $T$, are larger.

\section{memory-hardness}
\begin{proof}
    \cref{alg_ciphart} is just a variation of \emph{argon2d}, except that
    it uses an encryption function, $\enc$, instead of a hashing functionn.
    so if \emph{argon2d} is memory-hard, then so is \emph{ciphart}.
\end{proof}

\section{summary}

\end{document}
