\documentclass[twocolumn]{article}
\usepackage[margin=.7in]{geometry}
\usepackage[showisoZ=false]{datetime2}
\usepackage{graphicx}
\usepackage{url}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{security}{security interpretation}
\newtheorem{definition}{definition}
\newtheorem{theorem}{theorem}
\newtheorem{discovery}{discovery}
\newtheorem{advertisement}{advertisement}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\DeclareMathOperator{\fexists}{\mathtt{exists}}
\DeclareMathOperator{\fread}{\mathtt{read}}
\DeclareMathOperator{\fwrite}{\mathtt{write}}
\DeclareMathOperator{\sizeof}{\mathtt{sizeof}}
\DeclareMathOperator{\delete}{\mathtt{delete}}
\DeclareMathOperator{\enc}{\mathtt{enc}}
\DeclareMathOperator{\dec}{\mathtt{dec}}
\DeclareMathOperator{\maxf}{max}
\DeclareMathOperator{\hash}{\mathtt{hash}}
\DeclareMathOperator{\rhash}{\mathtt{rhash}}
\DeclareMathOperator{\mhash}{\mathtt{mhash}}
\DeclareMathOperator{\argon}{\mathtt{argon2}}
\DeclareMathOperator{\ciphart}{\mathtt{ciphart}}
\DeclareMathOperator{\cost}{\mathtt{cost}}
\renewcommand{\contentsname}{paper's layout}
\DTMsetdatestyle{iso}
\usepackage{cleveref} % must be loaded last
\begin{document}
\SetAlgorithmName{algorithm}{}{list of algorithms}
\SetInd{.15em}{1em}

\begin{center}
\Huge
ciphart\\
\LARGE
memory-\emph{harder} key derivation \\
with easier security interpretation\\
\normalsize
\emph{caveman}\footnote{mail: \texttt{toraboracaveman [at] protonmail [dot]
com}.}\\
\footnotesize
\DTMnow\\
\end{center}

\noindent\textbf{synopsis---}\emph{argon2}\footnote{\url{https://github.com/P-H-C/phc-winner-argon2}}
is mostly a nice memory-hard key derivation function, as it is simple
compared to
\emph{scrypt}\footnote{\url{http://www.tarsnap.com/scrypt/scrypt.pdf}},
which makes understanding what it is doing easier.  but i think that
\emph{argon2} is still not nice enough, as i think that its security
contribution can be simpler to understand, and i think that it can have
\emph{harder} memory-hardness.

currently, if you want to know \emph{argon2}'s contribution to your
security against password brute-forcing, you need to survey the industry of
application-specific integrated-circuits (asics) in order to find the cost
of breaking your password in a given time frame, as done in the
\emph{scrypt} paper.  the housekeeping of this approach is too expensive as
the industry is constantly changing, plus it remains a rough estimate with
inadequate theoretical guarantees.

also, \emph{argon2}'s memory-hardness is based on only utilising
random-access memories, and not, say, hard-disks.  without changing the
threat model of the vast majority of users, this is an unnecessary
restriction as i show in this paper.

henceforth, i propose \emph{ciphart} --- a memory-\emph{harder} key
derivation function, with:
\begin{itemize}
    \item a security contribution that is measured in the unit of shannon's
    entropy.  i.e. when \emph{ciphart} derives a key for you, it tells you
    that it has \emph{injected} shannon's entropy bits into your derived
    key, and also tells you its exact quantity.  this is possible thanks to
    my invention ``perfect lie'' theorem.
    \item and a memory-\emph{harder}ness that comes from the fact that
    \emph{ciphart} can require crazy amounts of memory, beyond our
    random-access memory, thanks to it being able to use the hard-disk as
    well.  this is possible thanks to my discovery ``cacheable keys''.

    this is optional, but i extremely like it as it effectively gives me
    much more security while eventually becoming much faster as well, and
    the adversary cannot get my cache even if he steals my hard-disks.
\end{itemize}

\texttt{libciphart}\footnote{\url{https://github.com/Al-Caveman/libciphart}}
is a library that implements \emph{ciphart} very closely to this paper,
without much fluff.  this should make integrating \emph{ciphart} into other
systems more convenient.

\texttt{ciphart}\footnote{\url{https://github.com/Al-Caveman/ciphart}} is
an application for encrypting and decrypting files that makes use of
\texttt{libciphart}.  this application is intended for use by end-users or
scripts, henceforth it has some fluff to treat mankind with dignity.

\break
\tableofcontents

\section{background}
we've got password $p$ with $H(p)$ many shannon's entropy bits worth of
information in it.  so what does this mean?

fundamentally, it means that, on average, we'd need to ask $H(p)$ many
perfect binary questions\footnote{one which, if answered, and on average,
gets the search space reduced in half.} in order to fully resolve all
ambiguities about $p$; i.e.  to fully get every bit of $p$.  

but people use it to do less orthodox things, such as quantifying the
amount of security $p$ has against, say, brute-forcing attacks.

say that we've got a $8V$ bit key $k \gets \hash(p \Vert s, 8V)$, derived
from password $p$, where $s$ is a salt.  say that the attacker has $s$ and
$k$ but wants to figure out $p$.  in this case, he will need to brute-force
the password space in order to find $p$ that gives $k$.  his cost is:
\begin{equation}\label{eq_cost_passbruteforce}
    2^{H(p)} \left(
        \cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}

\begin{definition}
the security of a system is the cost of the cheapest method that can break
it.
\end{definition}

one way to estimate $\cost$ is to survey the asics industry.  by surveying
the asics industry to get an idea how much money it costs to get a given
key, or password, space brute-forced within a target time
frame\footnote{see the \emph{scrypt} paper for an example.}.  this has an
expensive housekeeping and is usually not possible to get any guarantees as
we don't know about state-of-art manufacturing secrets that adversaries may
have.

another way is to ignore anything that has no cryptographic guarantee.  so,
in (\ref{eq_cost_passbruteforce}), cryptography
guarantees\footnote{statistically by confidence earned through peer review
and attempts to break encryption algorithms.} that $2^{H(p)}$ many $\hash$
calls are performed and that many equality tests.  the $\hash$ call needs
to be done once, so let's give it a unit of time $1$.  the equality test
also needs to be called once, but since since it's so cheap it's easier to
just assume that its cost is free.  this way (\ref{eq_cost_passbruteforce})
becomes just:
\begin{equation}\label{eq_simplecost_passbruteforce}
    2^{H(p)} (1+0) = 2^{H(p)}
\end{equation}

further, for convenience, it seems that people report it in the $\log_2$
scale.  i.e. $\log_2 2^{H(p)} = H(p)$.  i think this is why people use
password entropy as a measure of its security.  not because it is the
quantity of security, but rather because its the quantity of
\emph{simplified} security.  

i like this shannon's entropy-based simplified security quantity, so i'm
going to build on it.

\section{caveman's entropy}
\subsection{recursive $\hash$}
if the $\hash$ function is replaced by an $N$-deep recursion over $\hash$,
like:
\[
    \begin{split}
        & \rhash(p \Vert s, 8V, N) \\
    ={} &  \hash(\hash(\ldots\hash(p \Vert s, 8V), \ldots, 8V), 8V)
    \end{split}
\]
then, if $\hash$ is not broken,  (\ref{eq_cost_passbruteforce}) becomes:
\begin{equation}\label{eq_cost_passbruteforce_N}
    2^{H(p)} \left(
        N\cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}
(\ref{eq_simplecost_passbruteforce}) becomes:
\begin{equation}\label{eq_simplecost_passbruteforce_N}
    \begin{split}
    2^{H(p)} (N+0) &= N2^{H(p)} \\
                  &= 2^{H(p) + \log_2 N}
    \end{split}
\end{equation}
and the $\log_2$-scaled becomes $H(p) + \log_2 N$.

at this point, thanks to cryptographic guarantees concerning properties of
hashing functions, there is absolutely no security distinction from
adversary's point of view between a password with shannon's $H(p) + \log_2
N$ entropy bits, and a password with just $H(p)$ entropy bits that made use
of the $N$-deep recursive calls of $\hash$.

shannon's entropy of $p$ remains $H(p)$, but thanks to the recursive calls
of $\hash$, that password will be as expensive as another password $\hat
p$, such that $H(\hat p) = H(p) + \log_2 N$.

i think it will be simpler if we introduce the function-dependent caveman's
entropy $C$ as a measure.  it goes like this:
\begin{equation}
    C\Big(p, \hash(\ldots)\Big) = H(p)
\end{equation}

\begin{equation}
    C\Big(\hat p, \hash(\ldots)\Big) = H(p) + \log_2 N
\end{equation}

\begin{equation}
    \begin{split}
        C\Big(p, \rhash(\ldots, N)\Big) &= H(p) + \log_2 N \\
                                &= H(\hat p) \\
    \end{split}
\end{equation}

security-wise, there is no distinction between the more complex password
$\hat p$, and the simpler password $p$ that used $\rhash(\ldots, N)$.  so i
really think we need to measure password security in $C$ instead of $H$.

\subsection{memory-hard $\hash$}
let $\mhash$ be like $\rhash$, except that it also requires $M$ many memory
bytes such that, as available memory is linearly reduced from $M$, penalty
in cpu time grows exponentially.  let $M$ be requested memory, $A$ be
available memory, and $e(M - A)$ be the exponential penalty value for
reduction in memory, where $e(0) = 1$.
\begin{equation}
    \begin{split}
        & \cost\Big(\mhash(p \Vert s, N, M)\Big) \\
    ={} & \cost\Big(\rhash(p \Vert s, N)\Big)^{e(M-A)}
    \end{split}
\end{equation}

if $\hash$ in (\ref{eq_cost_passbruteforce}) is replaced by the $M$-bytes
memory-hardened $N$-deep recursion hash function $\mhash$, then
(\ref{eq_cost_passbruteforce}) becomes:
\begin{equation}\label{eq_cost_passbruteforce_NM}
    2^{H(p)} \left(
        N^{e(M-A)}\cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}
(\ref{eq_simplecost_passbruteforce}) becomes:
\begin{equation}\label{eq_simplecost_passbruteforce_NM}
    \begin{split}
    2^{H(p)} (N^{e(M-A)}+0) &= N^{e(M-A)} 2^{H(p)} \\
                  &= 2^{H(p) + \log_2 N^{e(M-A)}} \\
                  &= 2^{H(p) + e(M-A)\log_2 N}
    \end{split}
\end{equation}
and caveman's entropy becomes:
\begin{equation}
    C\Big(p, \mhash(\ldots, N, M)\Big) = H(p) + e(M-A)\log_2 N
\end{equation}

\section{``perfect lie'' theorem}
let $p$ be a password with $H(p)$ shannon's entropy bits.  let $\hat p$ be
a more complex password with $H(p) + e(M-A)\log_2 N$ shannon's entropy
bits, where $M$, $A$ and $N$ are all positive numbers.

then caveman's entropy says that the following keys are information
theoretically indistinguishable for as long as only $p$ and $\hat p$ remain
unknown (everything else is known, such as the distribution from which $p$
and $\hat p$ was sampled), and for as long as $\hash$ is not broken:
\begin{itemize}
    \item $k \gets \mhash(p \Vert s, N, M)$
    \item $\hat k \gets \hash(\hat p \Vert s)$
\end{itemize}

in other words:
\begin{equation}
    C\Big(p, \mhash(\ldots, N, M)\Big) = H(\hat p)
\end{equation}

since the assumption that passwords are kept away from the adversary is
fundamental in a symmetric encryption context, i think it makes since that
we measure our security with memory-hard key derivation functions using the
caveman's entropy $C$ instead of shannon's entropy $H$.

from a security point of view, it will feel absolutely identical to as if
the password got injected with extra shannon's entropy bits.  no one can
tell the difference for as long as the fundamental assumption of hiding
passwords is honoured, as well as the hashing function $\hash$ is not
broken.

in other words, we can say, if password $p$ is unknown, and $\hash$ is not
broken, then we have injected into $p$ extra shannon's entropy bits.  this
lie will be only discovered after $p$ is revealed.

if you think that it is impossible for this \emph{lie} to be \emph{truth}
under the secrecy of $p$, then i've done an even better job: proving that
cryptographically secure hashing functions do not exist.  likewise, same
can be trivially extended to: cryptographically symmetric ciphers do not
exist.

so you have to pick only one of these options:
\begin{enumerate}
    \item either accept that the lie is truth.  i.e. accept that we've
    injected shannon's entropy bits into $p$, for as long as only $p$ is
    not revealed.
    \item or, accept that cryptographically-secure hashing and
    symmetric-encryption functions functions do not exist.
\end{enumerate}

\begin{theorem}
    [perfect lie\footnote{i call \cref{theorem_perfect_lie} \emph{ perfect
    lie} in a sense that a perfect lie is indistinguishable from truth.}]
    \label{theorem_perfect_lie}
    for as long as password $p$ remains a secret and $\hash$ is not broken,
    derived keys $k_1 = \rhash(p, \ldots, N)$ and $k_2 = \mhash(p, \ldots,
    N, M)$ will have their shannon's entropies, $H(k_1)$ and $H(k_2)$,
    increase beyond $H(p)$ in order to equate their caveman's entropies
    $C(p, \rhash(\ldots, N))$ and $C(p, \mhash(\ldots, N, M))$,
    respectively.
\end{theorem}

the reason this lie is appealing is because it simplifies our
quantification of the amount of security that we have gained by using a
given key derivation function, such as $\rhash$ or $\mhash$.

without treating this lie as truth, our only hope would be surveying the
asics industry.  but with this lie, we have one more approach to get a feel
of the gained security quantity by just accepting caveman's entropy $C$ as
shannon's entropy $H$, and move on as if the lie is truth, and no one can
notice it.

we can also look at it from the perspective of \emph{occam's razor}.  i.e.
if two things are not distinguishable from one another, then assuming that
they are just the same thing is simpler than assuming otherwise.  

to be more specific about \emph{occam's razor}: (1) each assumption bit has
a positive probability of error by definition, (2) since assuming that
indistinguishable things are different than one another is more complex
(i.e. more assumption bits) than assuming not, and (3) since there is no
observable difference between the two things, therefore it necessarily
follows that our model's total error will be reduced if we accept that the
indistinguishable things are identical (i.e.  which is what
\cref{theorem_perfect_lie} says).

\section{``cacheable keys'' discovery}
\begin{discovery}[cacheable keys]\label{discov_key_caching}
    caching keys securely is easily doable, and great security utility
    exists in doing so for expensively-derived keys.
\end{discovery}

\subsection{why does it work}
\begin{itemize}
    \item when expensively derived keys are cached, only the first key
    derivation call will be expensive, while subsequent calls will be
    semi-instantaneous.  this effectively allows users to tolerate much
    more expensive, or secure, key derivation as it only happens during the
    initial, say, login phase.  subsequent use of the extremely expensive
    key is instantaneous.

    so instead of having the user use a somewhat expensive key derivation
    by wayting say, $3$ seconds in each login, he will ---instead--- wait,
    say $10$ seconds in his initial login in order to utilise a much more
    expensive key derivation, and then wait near $0$ seconds for every
    subsequent login as the expensive key is cached.

    \item derived keys can be cached securely, without increasing most
    users' assumptions.  e.g. cached keys can live in a \emph{dm-crypt}
    partition that is encrypted with a large encryption key that is stored
    properly, and the cache can have strict read permissions so that only
    the unique user that runs \texttt{ciphart} executable can read it.

    this only requires to trust the user \texttt{root}, which is already
    trusted by almost everyone.  so we are not introducing a new
    assumption.  
\end{itemize}

for most people, if \texttt{root} is compromised, then the adversary
can break every other key derivation function, including those that do
not cache keys, by simply, say, running a keylogger.

so, practically, we are not increasing the assumptions, but we are only
increasing the value that we can extract from the assumptions that we
already have.

most importantly, utilising discovery \ref{discov_key_caching} allows us to
achieve a memory-\emph{harder} key derivation in an extremely  usable way.
more details on memory-harderness later.

\subsection{potential adversary strategies}
let's see what may the adversary try to do against a key caching system:
\begin{itemize}
    \item \textbf{adversary strategy 1:} hack into user's system and execute a
    program as that user that tries to read the password cache file to
    obtain the memory-harder key inside it.

    \textbf{answer:} he will not be able to read the file due to strict
    read permissions of the cached files as set earlier.

    \item \textbf{adversary strategy 2:} steal user's hard disk and try to
    mount it in his system, to login as root and chance permissions of the
    key cache files.

    \textbf{answer:} the partition where the cached files are saved are
    properly encrypted, so he can't see the cached files, let alone
    changing their read permissions.

    \item \textbf{adversary strategy 3:} break into machine's \texttt{root}
    account.

    \textbf{answer:} he will succeed, but then he can also run a keylogger,
    which will also break every other key derivation function, even those
    who do not use key caching, such as \emph{scrypt}, \emph{argon2}, etc.
\end{itemize}

\section{application scenarios}
\subsection{a currently-useful scenario}
user has a password manager which generates unique $256$ bit entropy keys for
each online service that he uses.  the user also renews keys for his online
services every now and then.   so his online accounts generally have high
security.  

but user's problem is how to lock and unlock his password manager's
passwords database.  should he use a physical usb-stick key that types a
high-entropy key that encrypts and decrypts the database?   the user
doesn't want this physical key because of several reasons:
\begin{itemize}
    \item he tends to lose his keys a lot, and, for certain tasks, the risk
    of needing to wait for until he gets a backup usb-stick key is too
    much.

    \item he doesn't want to be caught having cryptographic usb-stick keys,
    because as such is an evidence that he has encrypted content.  the user
    wants to have the choice to lie that he has no clue.  so not having
    usb-stick keys with him helps his case to lie.

    \item he wants to be torture-resistant so that an adversary cannot
    forcefully take his keys from him in order to login into his services.
    he may rather want to die than to give the password to the adversary.
    this works because, so far, the brain is a pretty private information
    store.
\end{itemize}

in this scenario, the user memorises a sensible password that he can
remember, with enough initial entropy, and then uses \emph{ciphart},
preferably with disk caching, to inject large amounts of entropy bits into
his derived keys, way beyond the reach of pre-\emph{ciphart} key derivation
functions; thanks to \cref{theorem_perfect_lie} and discovery
\ref{discov_key_caching}.  

here, the expensively derived key is only used to unlock a local password
manager, which offers a protection against situations where a backup copy
of the passwords database is stolen.  this may enable the user to store his
passwords manager in an online file synchronisation service for more
convenient system migrations to further reduce his login delays should he
face the need to migrate to a new, say, personal computer.

\begin{advertisement}
    in case you're interested in such a passwords manager, i've also made
    \emph{nsapass}\footnote{\url{https://github.com/Al-Caveman/nsapass}}
    --- a flexible and a simple passwords manager in a few hundreds of
    python lines of code, that uses \emph{ciphart} by default.  i think
    this is the best command-line interfacing passwords manager by far, for
    its usability, and for the fact that auditing it is easy, thanks for it
    being only in a few hundreds of python lines.
\end{advertisement}

\subsection{a later-useful scenario}
all input password fields will internally call \texttt{libciphart} to
derive more expensive keys.  this way, applications, such as
\emph{firefox}, \emph{mutt}, \ldots, will never send actual passwords, but
will only send \emph{ciphart}-derived keys with increased shannon's entropy
content.

thanks to \cref{theorem_perfect_lie}, this will automatically increase
shannon's entropy of all users password without requiring users to memorise
harder passwords.  thanks to discovery \ref{discov_key_caching}, the user
will not face any delay in his daily use.

i also think that it is \emph{generally} better to have expensive key
derivation functions in the client side as opposed to the server side.
because remote servers always have the incentive to reduce the complexity
of the key derivation function in order to free more resources for other
things that bring them money.

either  way, \emph{ciphart} is perfectly usable on the server side.  it is
just that i think \emph{ciphart}, \emph{argon2}, \emph{scrypt}, \ldots make
better sense when placed on the client side.

\section{ciphart}
\subsection{parameters}
\begin{tabularx}{\columnwidth}{lX}
    $P$ & password.\\
    $S$ & salt.\\
    $M$ & total random-access memory in bytes.\\
    $D$ & total hard-disk memory in bytes.\\
    $F$ & temporary file's path.\\
    $Y$ & whether key caching is enabled.\\
    $L$ & number of memory lanes for concurrency.\\
    $T$ & number of tasks per lane segment.\\
    $H$ & number of lane segments per hard-disk read.\\
    $B$ & minimum \emph{caveman's entropy bits} to inject into $p$.\\
    $K$ & output key's size in bytes.\\
\end{tabularx}

\subsection{internal variables}
\begin{tabularx}{\columnwidth}{lX}
    $\enc$      & encryption function.\\
    $\hash$     & hashing function.\\
    $\fread$    & hard-disk file reading function, with seeking.  e.g.
                    $\fread(x, y, z)$ reads $z$ many bytes from file $x$
                    after seeking $y$ bytes forward.\\
    $\fwrite$   & hard-disk file writing function.\\
    $C$         & $\gets \begin{cases}
                        64 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is \emph{aes}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this to reflect the block size of the encryption
                    algorithm that implements $\enc$.\\
\end{tabularx}
\begin{tabularx}{\columnwidth}{lX}
    $V$ & $\gets \begin{cases}
                        32 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-128}}\\
                        32 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-256}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this is the size of the encryption key that's used to
                    solve \emph{ciphart}'s tasks.  this is different than
                    the $\enc$-independent $K$ which is
                    possibly used by other encryption algorithms in later
                    stages\footnote{at the expense of losing the meaning of
                    \emph{caveman's entropy bits}.}.\\
    $\hat T$    & $\gets \maxf(\lceil V C^{-1}\rceil, T)$.  this
                    is to ensure that we have enough encrypted bytes for
                    new keys.\\
    $\hat T$    & $\gets \hat T - (\hat T \bmod 2) + 2$.  this is to ensure
                    that there is an even number of tasks in a segment.
                    why?  because we need a buffer for storing the
                    clear-text and another for storing the output
                    cipher-text.\\
    $\hat M$    & $\gets M - (M \bmod C\hat TL) + C\hat TL$.  this is to
                    ensure that it is in multiples of $C\hat TL$.  why?  so
                    that all segments are of equal lengths in order to
                    simplify \emph{ciphart}'s logic.  e.g. it wouldn't be
                    nice if the last segments were of unequal sizes.\\
    $G$         & $\gets \hat MC^{-1}\hat T^{-1}L^{-1}$.  total number of
                    segments per lane.\\
\end{tabularx}
\begin{tabularx}{\columnwidth}{lX}
    $n$    & $\gets 0$.  actual number of times $\enc$ is called.\\
    $m_i$       & $C$-bytes memory for $i^{th}$ task in the $\hat M$-bytes
                    pad.\\
    $n_l$       & $\gets lG\hat T$.  nonce variable for $l^{th}$ lane with
                    at least $64$ bits.\\
    $f$         & $\gets 0$.  a counter indicating number of times
                    memory is filled with $\hat M$ many bytes.\\
    $d$         & $\gets 0$.  a counter indicating total number of saved
                    blocks into hard-disk.\\
    $h$         & $\gets 0$.  a counter indicating number of processed lane
                    segments since the last hard-disk read.\\
    $q$         & $\gets 0$.  a counter indicating number of times key was
                    updated from the hard-disk.\\
    $*v$         & $\gets *\hash(P \Vert S \Vert M \Vert D \Vert \ldots
                    \Vert K, V)$.  a pointer to the first byte where
                    $V$-bytes key is stored.  $v$ is the key itself.\\
    $Z$         & $\gets \hash(v \Vert 0, V)$.  file name where output key,
                    $k$, is expected to be cached, if $k$ was previously
                    cached.\\
\end{tabularx}

\subsection{output}
\begin{tabularx}{\columnwidth}{lX}
$k$ & $K$-bytes key.\\
$n$ & total number of times $\enc$ was actually called.  $\log_2 n$ is
        total number shannon's entropy bits that \emph{ciphart} injected
        into $k$, such that $\log_2 n \ge B$.\\
\end{tabularx}

\subsection{steps}
steps of \emph{ciphart} is shown in \cref{alg_ciphart}.  this corresponds
to \emph{argon2d}.  adding a \emph{ciphart-i} variant is a trivial matter,
i just didn't do it yet because my threat model currently doesn't benefit
from a password independent variant.

\begin{algorithm}
\If{$Y$ {\bf and} $\fexists(Z)$}{
    $k, n \gets \fread(Z, 0, V + \sizeof(n))$\;
    \Return{$k, n$}
}
\While{$1$}{
    \For{$g \gets 0, 1, \ldots, G-1$}{
        \For{$l \gets 0, 1, \ldots, L-1$}{\label{ciphart_lanes}
            \For{$t \gets 0, 1, \ldots, T-1$}{
                $i \gets gLT + lT + t$\;
                \uIf{$t < T - 1$}{
                    $j \gets i + 1$\;
                }\ElseIf{$t = T - 1$}{
                    $j \gets i - T + 1$\;
                }
                $m_j \gets \enc(m_i, n_l, *v)$\;
                $n_l \gets n_l + 1$\;
                \uIf{$f = 0$}{
                    $*v \gets m_j \bmod (gLTC + tC - V)$\;
                    \If{$*v \ge gLTC - V$}{
                        $*v \gets *v + lTC$\;
                    }
                }\Else{
                    $*v \gets m_j \bmod (\hat M - LTC + tC - V)$\;
                    \uIf{$*v \ge gLTC + tC - V$}{
                        $*v \gets *v + LTC$\;
                    }\ElseIf{$*v \ge gLTC - V$}{
                        $*v \gets *v + lTC$\;
                    }
                }
            }
        }
        $n \gets n + LT$\;
        \uIf{$d \le D$}{
            \For{$i \gets gLT, \ldots, gLT+(T-1)$}{
                $\fwrite(F, m_i)$\;
                $d \gets d + 1$\;
                \If{$d \ge D$}{ \textbf{break}\; }
            }
        }\Else{
            $h \gets h + L$\;
            \If{$h \ge H$}{
                $*v \gets *\fread(F, v \bmod (d - V), V)$\;
                $q \gets q + 1$\;
                $h \gets 0$\;
            }
            \If{$f \ge 2$ \bf and $q \ge 1$ \bf and $n \ge 2^B$}{
                $g_{\text{last}} \gets g$\;
                \textbf{go to} \cref{ciphart_out}\;
            }
        }
    }
    $f \gets f+1$\;
}
$i \gets g_{\text{last}}LT$\;\label{ciphart_out}
$k \gets \hash(m_{i+0T} \Vert m_{i+1T} \Vert \ldots \Vert m_{i+(L-1)T}, K)$\;
\If{$Y$}{
    $\fwrite(Z, v \Vert n)$\;
}
$\delete(F)$\;
\Return{$k$, $n$}
\caption{ciphart}
\label{alg_ciphart}
\end{algorithm}

\section{parallelism}
since iterations of the loop in \cref{ciphart_lanes} in \cref{alg_ciphart}
are fully independent of one other, they can quite happily utilise $L$ cpu
cores, specially when segment sizes, $T$, are larger.

\section{memory-hardness}
\begin{proof}
    \cref{alg_ciphart} is just a variation of \emph{argon2d}, except that
    it uses an encryption function, $\enc$, instead of a hashing functionn.
    so if \emph{argon2d} is memory-hard, then so is \emph{ciphart}.
\end{proof}


\section{memory-\emph{harder}ness}
thanks to discovery \ref{discov_key_caching}, memory-harderness is
possible.  this process goes like this:
\begin{enumerate}
    \item starts by recursively encrypting $D$ many bytes of some
    predefined sequence, such as zeros or \texttt{0xdbdb}\ldots, using a
    key derived from an cheaply-hashed password $p$.  by the end of this
    step, we have $D$ many key-based random sequence saved in the an
    encrypted hard disk partition.

    \item update the derived key to be the last $V$ bytes in the $D$ bytes
    file.

    \item move on to run a variant of \emph{argon2}, except that, every
    time $S$ many random-access memory lane segments are completed, some
    $V$ many bytes from $D$ are chosen randomly, not only based on the
    content the random-access memory, but also based on the $D$ bytes.
    this step makes \emph{ciphart} require $D+M$ bytes.

    \item delete the $D$ bytes, just to free up the disk space.
\end{enumerate}

if $S$ is large enough, this can be done efficiently without blocking the
cpu noticeably, as the randomly obtained $V$ bytes can be read by using
the $O(1)$ operation \texttt{seek} over the $D$ bytes.

\section{comparison}

\appendix
\section{donations}
this work is sponsored by a bright nerdiness that has unexpectedly sparkled
somewhere in an endless dark space.  nothing here was supposed to happen.
the cause  remains unknown, so we call it \emph{something random with large
entropy}.

ancient legend has it that, every time a donation is made for a good cause,
a beautiful torch is kindled somewhere deep within an otherwise cold and
dark space.

the torch is pure with an innocent smile every time a spending is committed
towards good.  but, she drops a tear when the spending is done in excess.
despite her attempts to maintain a steady light, sometimes her tears force
her light to flicker, and sometimes the flickering causes her to disappear.
when she is gone, she is never back again, except in the memories of those
who have once witnessed her charm\ldots

so, if you opt for a donation, i'd appreciate exercising wisdom, so that
the sensible balance is not exceeded; lest we want a beautiful torch to be
born, only to make her disappear in her tears.

\subsection{bitcoin}
\begin{center}
    \includegraphics[width=121px]{./pics/btc_wallet_address_trimmed.png}
    \texttt{bc1qtylzjtgd0yu4v7f8hyfzufn7nu692v9fc88jln}
\end{center}

\end{document}
