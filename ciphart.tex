\documentclass[twocolumn]{article}
\usepackage[margin=.7in]{geometry}
\usepackage[showisoZ=false]{datetime2}
\usepackage{url}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{security}{security interpretation}
\newtheorem{definition}{definition}
\newtheorem{theorem}{theorem}
\newtheorem{discovery}{discovery}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\DeclareMathOperator{\enc}{\mathtt{enc}}
\DeclareMathOperator{\dec}{\mathtt{dec}}
\DeclareMathOperator{\maxf}{max}
\DeclareMathOperator{\len}{len}
\DeclareMathOperator{\hash}{\mathtt{hash}}
\DeclareMathOperator{\rhash}{\mathtt{rhash}}
\DeclareMathOperator{\mhash}{\mathtt{mhash}}
\DeclareMathOperator{\argon}{\mathtt{argon2}}
\DeclareMathOperator{\ciphart}{\mathtt{ciphart}}
\DeclareMathOperator{\cost}{\mathtt{cost}}
\DeclareMathOperator{\henc}{\; HENC}
\DeclareMathOperator{\hhash}{\; HHASH}
\renewcommand{\contentsname}{paper's layout}
\makeatletter
\def\myrulefill{%
    \leavevmode\leaders\hrule%
    height .6ex width 1ex depth -0.4ex%
    \hfill\kern\z@%
}
\makeatother
\DTMsetdatestyle{iso}
\usepackage{cleveref} % must be loaded last
\begin{document}
\SetAlgorithmName{algorithm}{}{list of algorithms}
\SetInd{.15em}{1em}

\begin{center}
\Huge
ciphart\\
\LARGE
memory-\emph{harder} key derivation \\
with easier security interpretation\\
\normalsize
caveman\footnote{mail: toraboracaveman [at] protonmail [dot] com}\\
\footnotesize
\DTMnow\\
\end{center}

\noindent\textbf{synopsis---}argon2\footnote{\url{https://github.com/P-H-C/phc-winner-argon2}}
is mostly a nice memory-hard key derivation function, as it is simple
compared to
\emph{scrypt}\footnote{\url{http://www.tarsnap.com/scrypt/scrypt.pdf}},
which makes understanding what it is doing easier.  but i argue that
\emph{argon2} is still not nice enough, as it can be simpler and harder.

currently, if you want to know \emph{argon2}'s contribution to your
security against password brute-forcing, you need to survey the industry of
application-specific integrated-circuits (asics) in order to find the cost
of breaking your password in a given time frame, as done in the
\emph{scrypt} paper.  the housekeeping of this approach is too expensive as
the industry is constantly changing, plus it remains a rough estimate with
inadequate theoretical guarantees.

also, \emph{argon2}'s memory-hardness is based on only utilising
random-access memories, and not, say, hard-disks.  without changing the
threat model of the vast majority of users, this is an unnecessary
restriction as i show in this paper.

henceforth, i propose \emph{ciphart} --- a memory-\emph{harder} key
derivation function, with:
\begin{itemize}
    \item a security contribution that is measured in the unit of shannon's
    entropy.  i.e. when \emph{ciphart} derives its key, it also tells you
    the total quantity of the extra shannon's entropy bits that it has
    injected into your derived key.  this is possible thanks to my
    invention ``the perfect lie'' theorem.
    \item further, the memory-\emph{harder}ness comes from the fact that
    \emph{ciphart} can require crazy amounts of memory, beyond our
    random-access memory, thanks to it being able to use the hard-disk as
    well.  this is possible thanks to my discovery ``cacheable keys''.

    this is optional, but i extremely like it and use it, and the adversary
    cannot get my cache even if he steals my hard-disks.
\end{itemize}

\texttt{libciphart}\footnote{\url{https://github.com/Al-Caveman/libciphart}}
is a library that implements \emph{ciphart} very closely to this paper,
without much fluff.  this should make integrating \emph{ciphart} into other
systems more convenient.

\texttt{ciphart}\footnote{\url{https://github.com/Al-Caveman/ciphart}} is
an application for encrypting and decrypting files that makes use of
\texttt{libciphart}.  this application is intended for use by end-users or
scripts, henceforth it has some fluff to treat mankind with dignity.

\break
\tableofcontents

\section{background}
we've got password $p$ with $H(p)$ many shannon's entropy bits worth of
information in it.  so what does this mean?

fundamentally, it means that, on average, we'd need to ask $H(p)$ many
perfect binary questions\footnote{one which, if answered, and on average,
gets the search space reduced in half.} in order to fully resolve all
ambiguities about $p$; i.e.  to fully get every bit of $p$.  

but people use it to do less orthodox things, such as quantifying the
amount of security $p$ has against, say, brute-forcing attacks.

say that we've got a $8V$ bit key $k \gets \hash(p \Vert s, 8V)$, derived
from password $p$, where $s$ is a salt.  say that the attacker has $s$ and
$k$ but wants to figure out $p$.  in this case, he will need to brute-force
the password space in order to find $p$ that gives $k$.  his cost is:
\begin{equation}\label{eq_cost_passbruteforce}
    2^{H(p)} \left(
        \cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}

\begin{definition}
the security of a system is the cost of the cheapest method that can break
it.
\end{definition}

one way to estimate $\cost$ is to survey the asics industry.  by surveying
the asics industry to get an idea how much money it costs to get a given
key, or password, space brute-forced within a target time
frame\footnote{see the \emph{scrypt} paper for an example.}.  this has an
expensive housekeeping and is usually not possible to get any guarantees as
we don't know about state-of-art manufacturing secrets that adversaries may
have.

another way is to ignore anything that has no cryptographic guarantee.  so,
in (\ref{eq_cost_passbruteforce}), cryptography
guarantees\footnote{statistically by confidence earned through peer review
and attempts to break encryption algorithms.} that $2^{H(p)}$ many $\hash$
calls are performed and that many equality tests.  the $\hash$ call needs
to be done once, so let's give it a unit of time $1$.  the equality test
also needs to be called once, but since since it's so cheap it's easier to
just assume that its cost is free.  this way (\ref{eq_cost_passbruteforce})
becomes just:
\begin{equation}\label{eq_simplecost_passbruteforce}
    2^{H(p)} (1+0) = 2^{H(p)}
\end{equation}

further, for convenience, it seems that people report it in the $\log_2$
scale.  i.e. $\log_2 2^{H(p)} = H(p)$.  i think this is why people use
password entropy as a measure of its security.  not because it is the
quantity of security, but rather because its the quantity of
\emph{simplified} security.  

i like this shannon's entropy-based simplified security quantity, so i'm
going to build on it.

\section{caveman's entropy}
\subsection{recursive $\hash$}
if the $\hash$ function is replaced by an $N$-deep recursion over $\hash$,
like:
\[
    \begin{split}
        & \rhash(p \Vert s, 8V, N) \\
    ={} &  \hash(\hash(\ldots\hash(p \Vert s, 8V), \ldots, 8V), 8V)
    \end{split}
\]
then, if $\hash$ is not broken,  (\ref{eq_cost_passbruteforce}) becomes:
\begin{equation}\label{eq_cost_passbruteforce_N}
    2^{H(p)} \left(
        N\cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}
(\ref{eq_simplecost_passbruteforce}) becomes:
\begin{equation}\label{eq_simplecost_passbruteforce_N}
    \begin{split}
    2^{H(p)} (N+0) &= N2^{H(p)} \\
                  &= 2^{H(p) + \log_2 N}
    \end{split}
\end{equation}
and the $\log_2$-scaled becomes $H(p) + \log_2 N$.

at this point, thanks to cryptographic guarantees concerning properties of
hashing functions, there is absolutely no security distinction from
adversary's point of view between a password with shannon's $H(p) + \log_2
N$ entropy bits, and a password with just $H(p)$ entropy bits that made use
of the $N$-deep recursive calls of $\hash$.

shannon's entropy of $p$ remains $H(p)$, but thanks to the recursive calls
of $\hash$, that password will be as expensive as another password $\hat
p$, such that $H(\hat p) = H(p) + \log_2 N$.

i think it will be simpler if we introduce the function-dependent caveman's
entropy $C$ as a measure.  it goes like this:
\begin{equation}
    C\Big(p, \hash(\ldots)\Big) = H(p)
\end{equation}

\begin{equation}
    C\Big(\hat p, \hash(\ldots)\Big) = H(p) + \log_2 N
\end{equation}

\begin{equation}
    \begin{split}
        C\Big(p, \rhash(\ldots, N)\Big) &= H(p) + \log_2 N \\
                                &= H(\hat p) \\
    \end{split}
\end{equation}

security-wise, there is no distinction between the more complex password
$\hat p$, and the simpler password $p$ that used $\rhash(\ldots, N)$.  so i
really think we need to measure password security in $C$ instead of $H$.

\subsection{memory-hard $\hash$}
let $\mhash$ be like $\rhash$, except that it also requires $M$ many memory
bytes such that, as available memory is linearly reduced from $M$, penalty
in cpu time grows exponentially.  let $M$ be requested memory, $\hat M$ be
available memory, and $e(M - \hat M)$ be the exponential penalty value for
reduction in memory, where $e(0) = 1$.
\begin{equation}
    \begin{split}
        & \cost\Big(\mhash(p \Vert s, N, M)\Big) \\
    ={} & \cost\Big(\rhash(p \Vert s, N)\Big)^{e(\hat M - M)}
    \end{split}
\end{equation}

if $\hash$ in (\ref{eq_cost_passbruteforce}) is replaced by the $M$-bytes
memory-hardened $N$-deep recursion hash function $\mhash$, then
(\ref{eq_cost_passbruteforce}) becomes:
\begin{equation}\label{eq_cost_passbruteforce_NM}
    2^{H(p)} \left(
        N^{e(M-\hat M)}\cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}
(\ref{eq_simplecost_passbruteforce}) becomes:
\begin{equation}\label{eq_simplecost_passbruteforce_NM}
    \begin{split}
    2^{H(p)} (N^{e(M-\hat M)}+0) &= N^{e(M-\hat M)} 2^{H(p)} \\
                  &= 2^{H(p) + \log_2 N^{e(M-\hat M)}} \\
                  &= 2^{H(p) + e(M-\hat M)\log_2 N}
    \end{split}
\end{equation}
and caveman's entropy becomes:
\begin{equation}
    C\Big(p, \mhash(\ldots, N, M)\Big) = H(p) + e(M-\hat M)\log_2 N
\end{equation}

\section{``the perfect lie'' theorem}
let $p$ be a password with $H(p)$ shannon's entropy bits.  let $\hat p$ be
a more complex password with $H(p) + e(M-\hat M)\log_2 N$ shannon's entropy
bits, where $M$, $\hat M$ and $N$ are all positive numbers.

then caveman's entropy says that the following keys are information
theoretically indistinguishable for as long as only $p$ and $\hat p$ remain
unknown (everything else is known, such as the distribution from which $p$
and $\hat p$ was sampled), and for as long as $\hash$ is not broken:
\begin{itemize}
    \item $k \gets \mhash(p \Vert s, N, M)$
    \item $\hat k \gets \hash(\hat p \Vert s)$
\end{itemize}

in other words:
\begin{equation}
    C\Big(p, \mhash(\ldots, N, M)\Big) = H(\hat p)
\end{equation}

since the assumption that passwords are kept away from the adversary is
fundamental in a symmetric encryption context, i think it makes since that
we measure our security with memory-hard key derivation functions using the
caveman's entropy $C$ instead of shannon's entropy $H$.

from a security point of view, it will feel absolutely identical to as if
the password got injected with extra shannon's entropy bits.  no one can
tell the difference for as long as the fundamental assumption of hiding
passwords is honoured, as well as the hashing function $\hash$ is not
broken.

in other words, we can say, if password $p$ is unknown, and $\hash$ is not
broken, then we have injected into $p$ extra shannon's entropy bits.  this
lie will be only discovered after $p$ is revealed.

if you think that it is impossible for this \emph{lie} to be \emph{truth}
under the secrecy of $p$, then i've done an even better job: proving that
cryptographically secure hashing functions do not exist.  likewise, same
can be trivially extended to: cryptographically symmetric ciphers do not
exist.

so you have to pick only one of these options:
\begin{enumerate}
    \item either accept that the lie is truth.  i.e. accept that we've
    injected shannon's entropy bits into $p$, for as long as only $p$ is
    not revealed.
    \item or, accept that cryptographically-secure hashing and
    symmetric-encryption functions functions do not exist.
\end{enumerate}

\begin{theorem}
    [the perfect lie\footnote{i call \cref{theorem_perfect_lie} \emph{the
    perfect lie} theorem in a sense that a perfect lie is indistinguishable
    from truth.}]
    \label{theorem_perfect_lie}
when $p$ is secret and $\hash$ is not broken, then shannon's entropy $H$ of
the derived key equals caveman's entropy $C$.
\end{theorem}

the reason this lie is appealing is because it simplifies our
quantification of the amount of security that we have gained by using a
given key derivation function, such as $\rhash$ or $\mhash$.

without treating this lie as truth, our only hope would be surveying the
asics industry.  but with this lie, we have one more approach to get a feel
of the gained security quantity by just accepting caveman's entropy $C$ as
shannon's entropy $H$, and move on as if the lie is truth, and no one can
notice it.

we can also look at it from the perspective of \emph{occam's razor}.  i.e.
if two things are not distinguishable from one another, then assuming that
they are just the same thing is simpler than assuming otherwise.  

to be more specific about \emph{occam's razor}: (1) each assumption bit has
a positive probability of error by definition, (2) since assuming that
indistinguishable things are different than one another is more complex
(i.e. more assumption bits) than assuming not, and (3) since there is no
observable difference between the two things, therefore it necessarily
follows that our model's total error will be reduced if we accept that the
indistinguishable things are identical (i.e.  which is what
\cref{theorem_perfect_lie} says).

\section{``cacheable keys'' discovery}
\begin{discovery}[cacheable keys]\label{discov_key_caching}
    caching keys securely is easily doable, and great security utility
    exists in doing so for expensively-derived keys.
\end{discovery}

\subsection{why does it work}
\begin{itemize}
    \item when expensively derived keys are cached, only the first key
    derivation call will be expensive, while subsequent calls will be
    semi-instantaneous.  this effectively allows users to tolerate much
    more expensive, or secure, key derivation as it only happens during the
    initial, say, login phase.  subsequent use of the extremely expensive
    key is instantaneous.

    so instead of having the user use a somewhat expensive key derivation
    by wayting say, $3$ seconds in each login, he will ---instead--- wait,
    say $10$ seconds in his initial login in order to utilise a much more
    expensive key derivation, and then wait near $0$ seconds for every
    subsequent login as the expensive key is cached.

    \item derived keys can be cached securely, without increasing most
    users' assumptions.  e.g. cached keys can live in a \emph{dm-crypt}
    partition that is encrypted with a large encryption key that is stored
    properly, and the cache can have strict read permissions so that only
    the unique user that runs \texttt{ciphart} executable can read it.

    this only requires to trust the user \texttt{root}, which is already
    trusted by almost everyone.  so we are not introducing a new
    assumption.  
\end{itemize}

for most people, if \texttt{root} is compromised, then the adversary
can break every other key derivation function, including those that do
not cache keys, by simply, say, running a keylogger.

so, practically, we are not increasing the assumptions, but we are only
increasing the value that we can extract from the assumptions that we
already have.

most importantly, utilising discovery \ref{discov_key_caching} allows us to
achieve a memory-\emph{harder} key derivation in an extremely  usable way.
more details on memory-harderness later.

\subsection{potential adversary strategies}
let's see what can the adversary may try to do against a key caching
system:
\begin{itemize}
    \item \textbf{adversary strategy 1:} hack into user's system and execute a
    program as that user that tries to read the password cache file to
    obtain the memory-harder key inside it.

    \textbf{answer:} he will not be able to read the file due to strict
    read permissions of the cached files as set earlier.

    \item \textbf{adversary strategy 2:} steal user's hard disk and try to
    mount it in his system, to login as root and chance permissions of the
    key cache files.

    \textbf{answer:} the partition where the cached files are saved are
    properly encrypted, so he can't see the cached files, let alone
    changing their read permissions.

    \item \textbf{adversary strategy 3:} break into machine's \texttt{root}
    account.

    \textbf{answer:} he will succeed, but then he can also run a keylogger,
    which will also break every other key derivation function, even those
    who do not use key caching, such as \emph{scrypt}, \emph{argon2}, etc.
\end{itemize}

\subsection{an example scenario}
user has a password manager to generate unique $256$ bit entropy keys for
each service that he uses.  but user doesn't want to decrypt the password
manager's database using a physical dongle, so he cannot use a high-entropy
key for the purpose of unlocking the password manager.

why?  several possible reasons.  maybe (1) he tends to lose or forget
dongles and he doesn't want the delay associated with resolving such
physical dongle issues, maybe (2) not having a physical dongle will enhance
his repudiation case, or maybe (3) he wants to be torture-resistant so he
doesn't want to let the adversary take the physical dongle from him by
force\footnote{when elon musk's neural-link matures enough, we may end up
getting keys, and other information, pulled from our heads by force.  but,
until then, our brain seems to be a pretty secure information store.}.

in this case, the user memorises a sensible password that he can remember,
with enough initial entropy, and then uses \emph{ciphart} with disk caching
to inject large amounts of entropy bits into his derived keys, way beyond
the reach of pre-\emph{ciphart} key derivation functions; thanks to
\cref{theorem_perfect_lie} and discovery \ref{discov_key_caching}.  

\section{ciphart}
\subsection{parameters}
\begin{tabularx}{\columnwidth}{lX}
    $p$ & password.\\
    $s$ & salt.\\
    $M$ & total random-access memory in bytes.\\
    $D$ & total hard-disk memory in bytes.\\
    $L$ & number of memory lanes for concurrency.\\
    $T$ & number of tasks per lane segment.\\
    $S$ & number of lane segments per hard-disk read.\\
    $B$ & minimum \emph{caveman's entropy bits} to inject into $p$.\\
    $K$ & output key's size in bytes.\\
\end{tabularx}

\subsection{internal variables}
\begin{tabularx}{\columnwidth}{lX}
    $\enc$ & encryption function.\\
    $\hash$ & hashing function.\\
    $C$         & $\gets \begin{cases}
                        64 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is \emph{aes}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this to reflect the block size of the encryption
                    algorithm that implements $\enc$.\\
    $V$ & $\gets \begin{cases}
                        32 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-128}}\\
                        32 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-256}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this is the size of the encryption key that's used to
                    solve \emph{ciphart}'s tasks.  this is different than
                    the $\enc$-independent $K$ which is
                    possibly used by other encryption algorithms in later
                    stages\footnote{at the expense of losing the meaning of
                    \emph{caveman's entropy bits}.}.\\
    $\hat T$    & $\gets \maxf(\lceil V C^{-1}\rceil, T)$.  this
                    is to ensure that we have enough encrypted bytes for
                    new keys.\\
    $\hat T$    & $\gets \hat T - (\hat T \bmod 2) + 2$.  this is to ensure
                    that there is an even number of tasks in a segment.
                    why?  because we need a buffer for storing the
                    clear-text and another for storing the output
                    cipher-text.\\
    $\hat M$    & $\gets M - (M \bmod C\hat TL) + C\hat TL$.  this is to
                    ensure that it is in multiples of $C\hat TL$.  why?  so
                    that all segments are of equal lengths in order to
                    simplify \emph{ciphart}'s logic.  e.g. it wouldn't be
                    nice if the last segments were of unequal sizes.\\
    $\hat D$    & $\gets D - (D \bmod C) + C$.  this is to
                    ensure that it is in multiples of block sizes.\\
    $G$         & $\gets \hat MC^{-1}\hat T^{-1}L^{-1}$.  total number of
                    segments per lane.\\
    $N$    & $\gets 0$.  actual number of times $\enc$ is called,
                    where $\hat N \ge 2^B$.\\
    $m_i$       & $C$-bytes memory for $i^{th}$ task in the $\hat M$-bytes
                    pad.\\
    $n_l$       & $\gets lG\hat T$.  nonce variable for $l^{th}$ lane with
                    at least $64$ bits.\\
\end{tabularx}
\begin{tabularx}{\columnwidth}{lX}
    $f$         & $\gets 0$.  a flag indicating whether the $\hat M$-bytes
                    pad is filled.\\
    $v$         & $\gets *\hash(p \mathbin\Vert s, V)$.  a pointer to the
                    first byte where $V$-bytes key is stored.\\
\end{tabularx}

\subsection{output}
\begin{tabularx}{\columnwidth}{lX}
$k$ & $K$-bytes key.\\
$\hat B$ & actual \emph{caveman's entropy bits} that were injected into
            $p$, where $\hat B \ge B$.\\
\end{tabularx}

\subsection{steps}
steps of \emph{ciphart} is shown in \cref{alg_ciphart}.  this corresponds
to \emph{argon2d}.  adding a \emph{ciphart-i} variant is a trivial matter,
i just didn't do it yet because my threat model currently doesn't benefit
from a password independent variant.

\begin{algorithm}[tbh]
\While{$1$}{
    \For{$g=0, 1, \ldots, G-1$}{
        \For{$l=0, 1, \ldots, L-1$}{\label{ciphart_lanes}
            \For{$t=0, 1, \ldots, T-1$}{
                $i \gets gLT + lT + t$\;
                \uIf{$t < T - 1$}{
                    $j \gets i + 1$\;
                }\ElseIf{$t = T - 1$}{
                    $j \gets i - T + 1$\;
                }
                $m_j \gets \enc(m_i, n_l, v)$\;
                $n_l \gets n_l + 1$\;
                \uIf{$f = 0$}{
                    $v \gets m_j \bmod (gLTC + tC - V)$\;
                    \If{$v \ge gLTC - V$}{
                        $v \gets v + lTC$\;
                    }
                }\Else{
                    $v \gets m_j \bmod (\hat M - LTC + tC - V)$\;
                    \uIf{$v \ge gLTC + tC - V$}{
                        $v \gets v + LTC$\;
                    }\ElseIf{$v \ge gLTC - V$}{
                        $v \gets v + lTC$\;
                    }
                }
            }
        }
        $N \gets N + LT$\;
        \If{$N \ge 2^B$}{
            $g_{\text{last}} \gets g$\;
            \textbf{go to} \cref{ciphart_out}\;
        }
    }
    $f \gets 1$\;
}
$i \gets g_{\text{last}}LT$\;\label{ciphart_out}
$k \gets \hash(m_{i+0T} \Vert m_{i+1T} \Vert \ldots \Vert m_{i+(L-1)T}, K)$\;
$\hat B \gets log_2 N$\;
\Return{$k$, $\hat B$}
\caption{ciphart}
\label{alg_ciphart}
\end{algorithm}

\section{parallelism}
since iterations of the loop in \cref{ciphart_lanes} in \cref{alg_ciphart}
are fully independent of one other, they can quite happily utilise $L$ cpu
cores, specially when segment sizes, $T$, are larger.

\section{memory-hardness}
\begin{proof}
    \cref{alg_ciphart} is just a variation of \emph{argon2d}, except that
    it uses an encryption function, $\enc$, instead of a hashing functionn.
    so if \emph{argon2d} is memory-hard, then so is \emph{ciphart}.
\end{proof}


\section{memory-\emph{harder}ness}
thanks to discovery \ref{discov_key_caching}, memory-harderness is
possible.  this process goes like this:
\begin{enumerate}
    \item starts by recursively encrypting $D$ many bytes of some
    predefined sequence, such as zeros or \texttt{0xdbdb}\ldots, using a
    key derived from an cheaply-hashed password $p$.  by the end of this
    step, we have $D$ many key-based random sequence saved in the an
    encrypted hard disk partition.

    \item update the derived key to be the last $V$ bytes in the $D$ bytes
    file.

    \item move on to run a variant of \emph{argon2}, except that, every
    time $S$ many random-access memory lane segments are completed, some
    $V$ many bytes from $D$ are chosen randomly, not only based on the
    content the random-access memory, but also based on the $D$ bytes.
    this step makes \emph{ciphart} require $D+M$ bytes.

    \item delete the $D$ bytes, just to free up the disk space.
\end{enumerate}

if $S$ is large enough, this can be done efficiently without blocking the
cpu noticeably, as the randomly obtained $V$ bytes can be read by using
the $O(1)$ operation \texttt{seek} over the $D$ bytes.

\section{comparison}

\end{document}
