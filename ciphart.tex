\documentclass{article}
\usepackage[a4paper, total={7in, 8in}]{geometry}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[noend]{algorithmic}
\DeclareMathOperator{\enc}{enc}
\DeclareMathOperator{\maxf}{max}
\DeclareMathOperator{\nonce}{nonce}
\DeclareMathOperator{\len}{len}
\usepackage{url}
\renewcommand{\contentsname}{content}

\author{caveman}
\title{key derivation with easier measurable security}
\begin{document}
\begin{multicols}{2}
\maketitle

hi --- i propose \emph{ciphart}, a sequential memory-hard key derivation
function that has a security gain that's measurable more objectively and
more conveniently than anything in class known to date.

to nail this goal, \emph{ciphart}'s security gain is measured in the unit
of \emph{relative entropy bits}.  relative to what?  relative to the
encryption algorithm that's used later on.  therefore, this \emph{relative
entropy bits} measure is guaranteed to be true when the encryption
algorithm that's used with \emph{ciphart} is also the same one that's used
to encrypt the data afterwards.

my reference implementation is available
here\footnote{\url{https://github.com/Al-Caveman/ciphart}}.

\tableofcontents
\vfill\null
\columnbreak

\section{ciphart}
\noindent\textbf{parameters:}

\begin{tabular}{lp{18em}}
    $M$ & each task's size, at least $32$ bytes.\\
    $W$ & total memory in multiples of $2M$.\\
    $R$ & number of rounds per task.\\
    $B$ & added security in \emph{relative entropy bits}.\\
    $\enc$ & encryption function.\\
    $k$ & initial key.\\
\end{tabular}

\noindent\textbf{input:}

\begin{tabular}{lp{18em}}
    $T$ & $\gets W/M$\\
    $P$ & $\gets \maxf(2, \lceil2^B / (TR)\rceil)$\\
    $x$ & $\gets 0$, a $16$ bytes wide variable.\\
    $m_t$ & for any task $t \in \{1,2,\ldots,T\}$, $m_t$ is $M$-bytes memory
    for $t^{th}$ task to work on.  $m_t[0:16]$ means first $16$ bytes.
    $m_t[-16:]$ means last $16$ bytes.\\
    $\nonce$ & a variable with enough bytes to store nonces in.\\
\end{tabular}

\noindent\textbf{output:}

\begin{tabular}{lp{18em}}
$\hat k$ & better key, with $B$, or more, \emph{relative entropy bits}.
specifically, with $\log_2(PTR) \ge B$ bits.\\
\end{tabular}

\noindent\textbf{steps:}

\begin{algorithmic}[1]
    \FOR{$p=1, 2, \ldots, P$}
        \FOR{$l=1, 2, \ldots, L$}
            \FOR{$s=1, 2, \ldots, S$}
                \FOR{$t=1, 2, \ldots, T$}
                    \FOR{$r=1, 2, \ldots, R$}
                        \STATE $n \gets (p, l, s, t, r)$
                        \IF{$t = 1$}
                            \STATE $m_{l,s,t} \gets \enc(m_{l,s,T}, n, k)$
                        \ELSE
                            \STATE $m_{l,s,t} \gets \enc(m_{l,s,t-1}, n, k)$
                        \ENDIF
                        \STATE $k \gets f(m_{l,s,t}[-64:], p, l, s, t)$
                    \ENDFOR
                \ENDFOR
            \ENDFOR
        \ENDFOR
    \ENDFOR
    \WHILE{true}
        \FOR{$l=1, 2, \ldots, L$}
            \STATE $n \gets (0, l, 0, 0, 0)$
            \STATE $\hat k \gets \hat k \mathbin\Vert \enc(m_{l,S,T}[-64:], n, k)$
            \IF{$\len(\hat k) \ge K$}
                \RETURN $\hat k[0:K]$
            \ENDIF
        \ENDFOR
    \ENDWHILE
\end{algorithmic}
\vfill\null
\columnbreak

\section{parallelism}
iterations in steps $2$ to $12$, are independent of one another, so we can
distribute them happily across different threads to achieve maximum cpu
utilisation.

iterations in steps $13$ to $18$ can also be done in parallel after
completion of steps $2$ to $12$.

\section{hardness with smaller memory}
steps $13$ to $18$ is the part where sequential memory-hardness is expected
to be born.  proof maybe soon.  but first i have to actually test it in
code to see how fast/slow is it.  right now i fear that memory's I/O might
become a significant bottleneck.
\vfill\null
\columnbreak

\section{security interpretation}
\section{comparison}
\section{summary}

\end{multicols}
\end{document}
