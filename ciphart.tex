\documentclass[twocolumn]{article}
\usepackage[margin=.7in]{geometry}
\usepackage[showisoZ=false]{datetime2}
\usepackage{url}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{security}{security interpretation}
\newtheorem{definition}{definition}
\newtheorem{theorem}{theorem}
\newtheorem{note}{note}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\DeclareMathOperator{\enc}{\mathtt{enc}}
\DeclareMathOperator{\dec}{\mathtt{dec}}
\DeclareMathOperator{\maxf}{max}
\DeclareMathOperator{\len}{len}
\DeclareMathOperator{\hash}{\mathtt{hash}}
\DeclareMathOperator{\rhash}{\mathtt{rhash}}
\DeclareMathOperator{\mhash}{\mathtt{mhash}}
\DeclareMathOperator{\argon}{\mathtt{argon2}}
\DeclareMathOperator{\ciphart}{\mathtt{ciphart}}
\DeclareMathOperator{\cost}{\mathtt{cost}}
\DeclareMathOperator{\henc}{\; HENC}
\DeclareMathOperator{\hhash}{\; HHASH}
\renewcommand{\contentsname}{paper's layout}
\makeatletter
\def\myrulefill{%
    \leavevmode\leaders\hrule%
    height .6ex width 1ex depth -0.4ex%
    \hfill\kern\z@%
}
\makeatother
\DTMsetdatestyle{iso}
\usepackage{cleveref} % must be loaded last
\begin{document}
\SetAlgorithmName{algorithm}{}{list of algorithms}
\SetInd{.15em}{1em}

\begin{center}
\Huge
ciphart\\
\LARGE
memory-harder key derivation \\
with easier measurable security\\
\normalsize
caveman\footnote{mail: toraboracaveman [at] protonmail [dot] com}\\
\footnotesize
\DTMnow\\
\end{center}

\noindent\textbf{synopsis---}
argon2\footnote{\url{https://github.com/P-H-C/phc-winner-argon2}} is mostly
nice, as it is a simple memory-hard key derivation function relative to
\emph{scrypt}, which makes understanding what it is doing easier.  but i
argue that \emph{argon2} is still not nice enough, as it can be simpler and
harder.

currently, if you want to know what \emph{argon2} is giving for your
security-wise, you need to survey the industry of application-specific
integrated-circuits (asics) in order to obtain a cost-money map, as done in
the \emph{scrypt} paper.  the housekeeping of this approach is too
expensive as the industry is constantly changing, plus it remains a
rough estimate with inadequate theoretical guarantees.

henceforth, i propose \emph{ciphart} --- a memory-\emph{harder} key derivation
function, with a security contribution that is measured in the unit of
shannon's entropy.  i.e. \emph{ciphart} can say that it is injecting $x$
many shannon's entropy bits to your password.

\begin{itemize}
    \item the simpler security interpretation is thanks to \emph{the
    perfect lie} theorem that i discovered, which allows \emph{ciphart} to
    claim that it is injecting shannon's entropy bits into input passwords,
    for as long as the password remains a secret.

    the password remaining a secret for the adversary is a fundamental
    assumption of password-based security, hence the theorem applies
    against the vestiary, which is the side that it matters.

    \item the memory-harder\emph{ness} allows to require ridiculously large
    amounts of memory, way beyond our random-access memory.  this is done
    by utilising hard-disks, and can be done conveniently, in part, thanks
    to my discovery of the fact that caching derived keys into disks, is
    practical for almost every use cases.  

    it's optional, but i personally use it as it significantly enhances my
    security, and an adversary won't compromise my security, even if he
    steals my hard-disk.
\end{itemize}

\texttt{libciphart}\footnote{\url{https://github.com/Al-Caveman/libciphart}}
is a library that implements \emph{ciphart} very closely to this paper,
without much fluff.  this should make integrating \emph{ciphart} into other
systems more convenient.

\texttt{ciphart}\footnote{\url{https://github.com/Al-Caveman/ciphart}} is
an application for encrypting and decrypting files that makes use of
\texttt{libciphart}.  this application is intended for use by end-users or
scripts, henceforth it has some fluff to treat mankind with dignity.

\break
\tableofcontents

\section{background}
we've got password $p$ with $H(p)$ many shannon's entropy bits worth of
information in it.  so what does this mean?

fundamentally, it means that, on average, we'd need to ask $H(p)$ many
perfect binary questions\footnote{one which, if answered, and on average,
gets the search space reduced in half.} in order to fully resolve all
ambiguities about $p$; i.e.  to fully get every bit of $p$.  

but people use it to do less orthodox things, such as quantifying the
amount of security $p$ has against, say, brute-forcing attacks.

say that we've got a $8V$ bit key $k \gets \hash(p \Vert s, 8V)$, derived
from password $p$, where $s$ is a salt.  say that the attacker has $s$ and
$k$ but wants to figure out $p$.  in this case, he will need to brute-force
the password space in order to find $p$ that gives $k$.  his cost is:
\begin{equation}\label{eq_cost_passbruteforce}
    2^{H(p)} \left(
        \cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}

\begin{definition}
the security of a system is the cost of the cheapest method that can break
it.
\end{definition}

one way to estimate $\cost$ is to survey the asics industry.  by surveying
the asics industry to get an idea how much money it costs to get a given
key, or password, space brute-forced within a target time
frame\footnote{see the \emph{scrypt} paper for an example.}.  this has an
expensive housekeeping and is usually not possible to get any guarantees as
we don't know about state-of-art manufacturing secrets that adversaries may
have.

another way is to ignore anything that has no cryptographic guarantee.  so,
in (\ref{eq_cost_passbruteforce}), cryptography
guarantees\footnote{statistically by confidence earned through peer review
and attempts to break encryption algorithms.} that $2^{H(p)}$ many $\hash$
calls are performed and that many equality tests.  the $\hash$ call needs
to be done once, so let's give it a unit of time $1$.  the equality test
also needs to be called once, but since since it's so cheap it's easier to
just assume that its cost is free.  this way (\ref{eq_cost_passbruteforce})
becomes just:
\begin{equation}\label{eq_simplecost_passbruteforce}
    2^{H(p)} (1+0) = 2^{H(p)}
\end{equation}

further, for convenience, it seems that people report it in the $\log_2$
scale.  i.e. $\log_2 2^{H(p)} = H(p)$.  i think this is why people use
password entropy as a measure of its security.  not because it is the
quantity of security, but rather because its the quantity of
\emph{simplified} security.  

\section{caveman's entropy}
\subsection{recursive $\hash$}
if the $\hash$ function is replaced by an $N$-deep recursion over $\hash$,
like:
\[
    \begin{split}
        & \rhash(p \Vert s, 8V, N) \\
    ={} &  \hash(\hash(\ldots\hash(p \Vert s, 8V), \ldots, 8V), 8V)
    \end{split}
\]
then, if $\hash$ is not broken,  (\ref{eq_cost_passbruteforce}) becomes:
\begin{equation}\label{eq_cost_passbruteforce_N}
    2^{H(p)} \left(
        N\cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}
and (\ref{eq_simplecost_passbruteforce}) becomes:
\begin{equation}\label{eq_simplecost_passbruteforce_N}
    \begin{split}
    2^{H(p)} (N+0) &= N2^{H(p)} \\
                  &= 2^{H(p) + \log_2 N}
    \end{split}
\end{equation}

at this point, thanks to cryptographic guarantees concerning properties of
hashing functions, there is absolutely no security distinction between a
password with shannon's $H(p) + \log_2 N$ entropy bits, and a password with
just $H(p)$ entropy bits that made use of the $N$-deep recursive calls of
$\hash$.

shannon's entropy of $p$ remains $H(p)$, but thanks to the recursive calls
of $\hash$, that password will be as expensive as another password $\hat
p$, such that $H(\hat p) = H(p) + \log_2 N$.

i think it will be simpler if we introduce the function-dependent caveman's
entropy $C$ as a measure.  it goes like this:
\begin{equation}
    C\Big(p, \hash(\ldots)\Big) = H(p)
\end{equation}

\begin{equation}
    C\Big(\hat p, \hash(\ldots)\Big) = H(p) + \log_2 N
\end{equation}

\begin{equation}
    \begin{split}
        C\Big(p, \rhash(\ldots, N)\Big) &= H(p) + \log_2 N \\
                                &= H(\hat p) \\
    \end{split}
\end{equation}

security-wise, there is no distinction between the more complex password
$\hat p$, and the simpler password $p$ that used $\rhash(\ldots, N)$.  so i
really think we need to measure password security in $C$ instead of $H$.

\subsection{memory-hard $\hash$}
let $\mhash$ be like $\rhash$, except that it also requires $M$ many memory
bytes such that, as available memory is linearly reduced from $M$, penalty
in cpu time grows exponentially.  let $M$ be requested memory, $\hat M$ be
available memory, and $e(M - \hat M)$ be the exponential penalty value for
reduction in memory, where $e(0) = 1$.
\begin{equation}
    \begin{split}
        & \cost\Big(\mhash(p \Vert s, N, M)\Big) \\
    ={} & \cost\Big(\rhash(p \Vert s, N)\Big)^{e(\hat M - M)}
    \end{split}
\end{equation}

if $\hash$ in (\ref{eq_cost_passbruteforce}) is replaced by the $M$-bytes
memory-hardened $N$-deep recursion hash function $\mhash$, then
(\ref{eq_cost_passbruteforce}) becomes:
\begin{equation}\label{eq_cost_passbruteforce_NM}
    2^{H(p)} \left(
        N^{e(M-\hat M)}\cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}
(\ref{eq_simplecost_passbruteforce}) becomes:
\begin{equation}\label{eq_simplecost_passbruteforce_NM}
    \begin{split}
    2^{H(p)} (N^{e(M-\hat M)}+0) &= N^{e(M-\hat M)} 2^{H(p)} \\
                  &= 2^{H(p) + \log_2 N^{e(M-\hat M)}} \\
                  &= 2^{H(p) + e(M-\hat M)\log_2 N}
    \end{split}
\end{equation}
and caveman's entropy becomes:
\begin{equation}
    C\Big(p, \mhash(\ldots, N, M)\Big) = H(p) + e(M-\hat M)\log_2 N
\end{equation}

\section{the perfect lie theorem}
let $p$ be a password with $H(p)$ shannon's entropy bits.  let $\hat p$ be
a more complex password with $H(p) + e(M-\hat M)\log_2 N$ shannon's entropy
bits, where $M$, $\hat M$ and $N$ are all positive numbers.

then caveman's entropy says that the following keys are information
theoretically indistinguishable for as long as only $p$ and $\hat p$ remain
unknown (everything else is known, such as the distribution from which $p$
and $\hat p$ was sampled), and for as long as $\hash$ is not broken:
\begin{itemize}
    \item $k \gets \mhash(p \Vert s, N, M)$
    \item $\hat k \gets \hash(\hat p \Vert s)$
\end{itemize}

in other words:
\begin{equation}
    C\Big(p, \mhash(\ldots, N, M)\Big) = H(\hat p)
\end{equation}

since the assumption that passwords are kept away from the adversary is
fundamental in a symmetric encryption context, i think it makes since that
we measure our security with memory-hard key derivation functions using the
caveman's entropy $C$ instead of shannon's entropy $H$.

from a security point of view, it will feel absolutely identical to as if
the password got injected with extra shannon's entropy bits.  no one can
tell the difference for as long as the fundamental assumption of hiding
passwords is honoured, as well as the hashing function $\hash$ is not
broken.

in other words, we can say, if password $p$ is unknown, and $\hash$ is not
broken, then we have injected into $p$ extra shannon's entropy bits.  this
lie will be only discovered after $p$ is revealed.

if you think that it is impossible for this \emph{lie} to be \emph{truth}
under the secrecy of $p$, then i've done an even better job: proving that
cryptographically secure hashing functions do not exist.  likewise, same
can be trivially extended to: cryptographically symmetric ciphers do not
exist.

so you have to pick only one of these options:
\begin{enumerate}
    \item either accept that the lie is truth.  i.e. accept that we've
    injected shannon's entropy bits into $p$, for as long as only $p$ is
    not revealed.
    \item or, accept that cryptographically-secure hashing and
    symmetric-encryption functions functions do not exist.
\end{enumerate}

\begin{theorem}
    [the perfect lie\footnote{i call \cref{theorem_perfect_lie} \emph{the
    perfect lie} theorem in a sense that a perfect lie is indistinguishable
    from truth.}]
    \label{theorem_perfect_lie}
when $p$ is secret and $\hash$ is not broken, then shannon's entropy $H$ of
the derived key equals caveman's entropy $C$.
\end{theorem}

the reason this lie is appealing is because it simplifies our
quantification of the amount of security that we have gained by using a
given key derivation function, such as $\rhash$ or $\mhash$.

without treating this lie as truth, our only hope would be surveying the
asics industry.  but with this lie, we have one more approach to get a feel
of the gained security quantity by just accepting caveman's entropy $C$ as
shannon's entropy $H$, and move on as if the lie is truth, and no one can
notice it.

we can also look at it from the perspective of \emph{occam's razor}.  i.e.
if two things are not distinguishable from one another, then assuming that
they are just the same thing is simpler than assuming otherwise.  

to be more specific about \emph{occam's razor}: (1) each assumption bit has
a positive probability of error by definition, (2) since assuming that
indistinguishable things are different than one another is more complex
(i.e. more assumption bits) than assuming not, and (3) since there is no
observable difference between the two things, therefore it necessarily
follows that our model's total error will be reduced if we accept that the
indistinguishable things are identical (i.e.  which is what
\cref{theorem_perfect_lie} says).

\section{memory-harder\emph{ness}}
let's say that $D$ is total number of bytes for hard-disk-based memory
hardness.  this process goes like this:
\begin{enumerate}
    \item starts by recursively encrypting $D$ many bytes of some
    predefined sequence, such as zeros or \texttt{0xdbdb}\ldots, using a
    key derived from an cheaply-hashed password $p$.  by the end of this
    step, we have $D$ many key-based random sequence saved in the an
    encrypted hard disk partition.

    \item update the derived key to be the last $V$ bytes in the $D$ bytes
    file.

    \item move on to run a variant of \emph{argon2}, except that, every
    time $S$ many random-access memory lane segments are completed, some
    $V$ many bytes from $D$ are chosen randomly, not only based on the
    content the random-access memory, but also based on the $D$ bytes.
    this step makes \emph{ciphart} require $D+M$ bytes.

    if $S$ is large enough, this can be done efficiently without blocking
    the cpu noticeably, as the randomly obtained $V$ bytes can be read by
    using \texttt{seek} call over the $D$ bytes.

    \item delete the $D$ bytes, just to free up the disk space.
\end{enumerate}

so far this is a bad idea as it is not usable due to its slowness when $D$
is large.  but what makes it usable is these points:
\begin{itemize}
    \item we assume \texttt{root} user is not compromised.  the vast
    majority of systems already assume this.  so this is not a practical
    problem for the vast majority of us.

    \item hard disk partitions where files are saved is properly encrypted
    by things like \emph{dm-crypt} using a large random key file that is
    properly secured as whatever suitable standards.  this is technically a
    simple thing to do.  grandmother might find a difficulty doing this on
    her own, but she wouldn't need to do it by herself as her, say,
    \emph{gnu/linux} distribution may take care of this for her.

    \item the whole process of deriving a memory-harder key is done only
    once, as the derived key is cached in disk in an encrypted form with
    strict read permission, for long term use.  so subsequent calls of the
    \texttt{ciphart} executable will simply retrieve the cached object.

    \item the disk, where the cached object lives, is properly encrypted by
    using, say, \emph{dm-crypt}, and a large random key file.

    \item the \texttt{ciphart} executable has \texttt{SETUID} bit set, with
    its user being a unique user.  when \texttt{ciphart} writes a cached
    object, it saves it with permission mask \texttt{600} so that only the
    unique user can read it.  thanks to the \texttt{SETUID} bit, way any
    user can execute \texttt{ciphart}, and quickly get the memory-harder
    key only if he supplies the correct password.

    this way the user will deal with the system with extreme continence as
    if a cheap key derivation function is used, except for being a
    memory-harder derived key.  the user will only need to wait for $D$
    creation if his key caches are deleted, which doesn't happen except for
    once when the user migrates to a different machine.
    
    \item the \texttt{ciphart} executable sleeps if number of attempts
    exceeds a limit.  this will make brute-forcing the cache file by
    calling \texttt{ciphart} very expensive.
\end{itemize}

let's see what can the adversary may try to do:
\begin{itemize}
    \item \textbf{adversary strategy 1:} hack into user's system and execute a
    program as that user that tries to read the password cache file to
    obtain the memory-harder key inside it.

    \textbf{answer:} he will not be able to read the file due to strict
    read permissions of the cached files as set earlier.

    \item \textbf{adversary strategy 2:} steal user's hard disk and try to mount it
    in his system, to login as root and chance permissions of the key cache
    files.

    \textbf{answer:} the partition where the cached files are saved are
    properly encrypted, so he can't see the cached files, let alone
    changing their read permissions.

    \item \textbf{adversary strategy 3:} break into machine's \texttt{root} account.

    \textbf{answer:} he will succeed, but then he can also run a keylogger,
    which will also break every other key derivation function, even those
    who do not use key caching, such as \emph{scrypt}, \emph{argon2}, etc.
\end{itemize}

what we have gained is that the derived key is extremely fast to retrieve,
yet requires an amount of memory that $D+M$, where $M$ is the amount of
the required random-access memory.  memory-hard key derivation functions
can only require $M$, while \emph{ciphart} can require $D+M$ and still be
faster for later use.

if the system is setup properly, the only assumption that needs to be had
is that the account \texttt{root} is not compromised, which almost everyone
is assuming this already anyway.

so the idea of caching derived keys is just neat and allows to inject crazy
amounts of shannon's entropy bits into otherwise simple passwords, thanks
to this ridiculous memory-harder\emph{ness}.

\section{ciphart}
\subsection{parameters}
\begin{tabularx}{\columnwidth}{lX}
    $p$ & password.\\
    $s$ & salt.\\
    $M$ & total random-access memory in bytes.\\
    $D$ & total hard-disk memory in bytes.\\
    $L$ & number of memory lanes for concurrency.\\
    $T$ & number of tasks per lane segment.\\
    $S$ & number of lane segments per hard-disk read.\\
    $B$ & minimum \emph{caveman's entropy bits} to inject into $p$.\\
    $K$ & output key's size in bytes.\\
\end{tabularx}

\subsection{internal variables}
\begin{tabularx}{\columnwidth}{lX}
    $\enc$ & encryption function.\\
    $\hash$ & hashing function.\\
    $C$         & $\gets \begin{cases}
                        64 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is \emph{aes}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this to reflect the block size of the encryption
                    algorithm that implements $\enc$.\\
    $V$ & $\gets \begin{cases}
                        32 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-128}}\\
                        32 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-256}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this is the size of the encryption key that's used to
                    solve \emph{ciphart}'s tasks.  this is different than
                    the $\enc$-independent $K$ which is
                    possibly used by other encryption algorithms in later
                    stages\footnote{at the expense of losing the meaning of
                    \emph{caveman's entropy bits}.}.\\
    $\hat T$    & $\gets \maxf(\lceil V C^{-1}\rceil, T)$.  this
                    is to ensure that we have enough encrypted bytes for
                    new keys.\\
    $\hat T$    & $\gets \hat T - (\hat T \bmod 2) + 2$.  this is to ensure
                    that there is an even number of tasks in a segment.
                    why?  because we need a buffer for storing the
                    clear-text and another for storing the output
                    cipher-text.\\
    $\hat M$    & $\gets M - (M \bmod C\hat TL) + C\hat TL$.  this is to
                    ensure that it is in multiples of $C\hat TL$.  why?  so
                    that all segments are of equal lengths in order to
                    simplify \emph{ciphart}'s logic.  e.g. it wouldn't be
                    nice if the last segments were of unequal sizes.\\
    $\hat D$    & $\gets D - (D \bmod C) + C$.  this is to
                    ensure that it is in multiples of block sizes.\\
    $G$         & $\gets \hat MC^{-1}\hat T^{-1}L^{-1}$.  total number of
                    segments per lane.\\
    $N$    & $\gets 0$.  actual number of times $\enc$ is called,
                    where $\hat N \ge 2^B$.\\
    $m_i$       & $C$-bytes memory for $i^{th}$ task in the $\hat M$-bytes
                    pad.\\
    $n_l$       & $\gets lG\hat T$.  nonce variable for $l^{th}$ lane with
                    at least $64$ bits.\\
    $f$         & $\gets 0$.  a flag indicating whether the $\hat M$-bytes
                    pad is filled.\\
    $v$         & $\gets *\hash(p \mathbin\Vert s, V)$.  a pointer to the
                    first byte where $V$-bytes key is stored.\\
\end{tabularx}

\subsection{output}
\begin{tabularx}{\columnwidth}{lX}
$k$ & $K$-bytes key.\\
$\hat B$ & actual \emph{caveman's entropy bits} that were injected into
            $p$, where $\hat B \ge B$.\\
\end{tabularx}

\subsection{steps}
steps of \emph{ciphart} is shown in \cref{alg_ciphart}.  this corresponds
to \emph{argon2d}.  adding a \emph{ciphart-i} variant is a trivial matter,
i just didn't do it yet because my threat model currently doesn't benefit
from a password independent variant.

\begin{algorithm}[tbh]
\While{$1$}{
    \For{$g=0, 1, \ldots, G-1$}{
        \For{$l=0, 1, \ldots, L-1$}{\label{ciphart_lanes}
            \For{$t=0, 1, \ldots, T-1$}{
                $i \gets gLT + lT + t$\;
                \uIf{$t < T - 1$}{
                    $j \gets i + 1$\;
                }\ElseIf{$t = T - 1$}{
                    $j \gets i - T + 1$\;
                }
                $m_j \gets \enc(m_i, n_l, v)$\;
                $n_l \gets n_l + 1$\;
                \uIf{$f = 0$}{
                    $v \gets m_j \bmod (gLTC + tC - V)$\;
                    \If{$v \ge gLTC - V$}{
                        $v \gets v + lTC$\;
                    }
                }\Else{
                    $v \gets m_j \bmod (\hat M - LTC + tC - V)$\;
                    \uIf{$v \ge gLTC + tC - V$}{
                        $v \gets v + LTC$\;
                    }\ElseIf{$v \ge gLTC - V$}{
                        $v \gets v + lTC$\;
                    }
                }
            }
        }
        $N \gets N + LT$\;
        \If{$N \ge 2^B$}{
            $g_{\text{last}} \gets g$\;
            \textbf{go to} \cref{ciphart_out}\;
        }
    }
    $f \gets 1$\;
}
$i \gets g_{\text{last}}LT$\;\label{ciphart_out}
$k \gets \hash(m_{i+0T} \Vert m_{i+1T} \Vert \ldots \Vert m_{i+(L-1)T}, K)$\;
$\hat B \gets log_2 N$\;
\Return{$k$, $\hat B$}
\caption{ciphart}
\label{alg_ciphart}
\end{algorithm}

\section{parallelism}
since iterations of the loop in \cref{ciphart_lanes} in \cref{alg_ciphart}
are fully independent of one other, they can quite happily utilise $L$ cpu
cores, specially when segment sizes, $T$, are larger.

\section{memory-hardness}
\begin{proof}
    \cref{alg_ciphart} is just a variation of \emph{argon2d}, except that
    it uses an encryption function, $\enc$, instead of a hashing functionn.
    so if \emph{argon2d} is memory-hard, then so is \emph{ciphart}.
\end{proof}

\section{summary}

\end{document}
