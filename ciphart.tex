\documentclass{article}
\usepackage[a4paper, total={7in, 8in}]{geometry}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithmic}
\DeclareMathOperator{\enc}{enc}
\DeclareMathOperator{\maxf}{max}
\DeclareMathOperator{\nonce}{nonce}
\DeclareMathOperator{\hash}{hash}
\usepackage{url}
\renewcommand{\contentsname}{content}

\author{caveman}
\title{key derivation with easier measurable security}
\begin{document}
\begin{multicols}{2}
\maketitle

hi --- i propose \emph{ciphart}, a sequential memory-hard key derivation
function that has a security gain that's measurable more objectively and
more conveniently than anything in class known to date.

to nail this goal, \emph{ciphart}'s security gain is measured in the unit
of \emph{relative entropy bits}.  relative to what?  relative to the
encryption algorithm that's used later on.  therefore, this \emph{relative
entropy bits} measure is guaranteed to be true when the encryption
algorithm that's used with \emph{ciphart} is also the same one that's used
to encrypt the data afterwards.

my reference implementation is available
here\footnote{\url{https://github.com/Al-Caveman/ciphart}}.

\tableofcontents
\vfill\null
\columnbreak

\section{ciphart}
\noindent\textbf{parameters:}

\begin{tabular}{lp{18em}}
    $M$ & each task's size, at least $32$ bytes.\\
    $W$ & total memory in multiples of $2M$.\\
    $R$ & number of rounds per task.\\
    $B$ & added security in \emph{relative entropy bits}.\\
    $\enc$ & encryption function.\\
    $k$ & initial key.\\
\end{tabular}

\noindent\textbf{input:}

\begin{tabular}{lp{18em}}
    $T$ & $\gets W/M$\\
    $P$ & $\gets \maxf(2, \lceil2^B / (TR)\rceil)$\\
    $x$ & $\gets 0$, a $16$ bytes wide variable.\\
    $m_t$ & for any task $t \in \{1,2,\ldots,T\}$, $m_t$ is $M$-bytes memory
    for $t^{th}$ task to work on.  $m_t[0:16]$ means first $16$ bytes.
    $m_t[-16:]$ means last $16$ bytes.\\
    $\nonce$ & a variable with enough bytes to store nonces in.\\
    $\hash$ & a function to compress $W$ bytes into desired key length.\\
\end{tabular}

\noindent\textbf{output:}

\begin{tabular}{lp{18em}}
$\hat k$ & better key, with $B$, or more, \emph{relative entropy bits}.
specifically, with $\log_2(PTR) \ge B$ bits.\\
\end{tabular}

\noindent\textbf{steps:}

\begin{algorithmic}[1]
    \FOR{$p=1, 2, \ldots, P$}
        \FOR{$t=1,3, \ldots, T-1$, in steps of $2$}
            \STATE $i \gets t$
            \STATE $j \gets t+1$
            \FOR{$r=1, 2, \ldots, 2R$}
                \STATE $\nonce \gets (p, t, r)$
                \STATE $m_i \gets \enc(m_j, \nonce, k)$
                \STATE $\hat i \gets i$
                \STATE $i \gets j$
                \STATE $j \gets \hat i$
            \ENDFOR
        \ENDFOR
        \FOR{$t=1,2, \ldots, T$}
            \FOR{$\hat t=1, \ldots, t-1$ and $\hat t=t+1, \ldots, T$}
                \STATE $m_t[-16:] \gets m_t[-16:] \ll 1$
                \STATE $m_t[-16:] \gets m_t[-16:] \oplus m_{\hat t}[0:16]$
            \ENDFOR
        \ENDFOR
    \ENDFOR
    \RETURN $\hat k \gets \hash(m_1, m_2, \ldots, m_T)$
\end{algorithmic}
\vfill\null
\columnbreak

\section{parallelism}
iterations in steps $2$ to $12$, are independent of one another, so we can
distribute them happily across different threads to achieve maximum cpu
utilisation.

iterations in steps $13$ to $18$ can also be done in parallel after
completion of steps $2$ to $12$.

\section{sequential-memory hardness}
steps $13$ to $18$ is the part where sequential memory-hardness is expected
to be born.  proof maybe soon.  but first i have to actually test it in
code to see how fast/slow is it.  right now i fear that memory's I/O might
become a significant bottleneck.
\vfill\null
\columnbreak

\section{security interpretation}
\section{comparison}
\section{summary}

\end{multicols}
\end{document}
