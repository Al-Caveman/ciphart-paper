\documentclass[twocolumn]{article}
\usepackage[margin=.7in]{geometry}
\usepackage[showisoZ=false]{datetime2}
\usepackage{graphicx}
\usepackage{url}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{security}{security interpretation}
\newtheorem{definition}{definition}
\newtheorem{lemma}{lemma}
\newtheorem{theorem}{theorem}
\newtheorem{discovery}{discovery}
\newtheorem{law}{law}
\newtheorem{advertisement}{advertisement}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\DeclareMathOperator{\fexists}{\mathtt{exists}}
\DeclareMathOperator{\fread}{\mathtt{read}}
\DeclareMathOperator{\fwrite}{\mathtt{write}}
\DeclareMathOperator{\sizeof}{\mathtt{sizeof}}
\DeclareMathOperator{\delete}{\mathtt{delete}}
\DeclareMathOperator{\enc}{\mathtt{enc}}
\DeclareMathOperator{\dec}{\mathtt{dec}}
\DeclareMathOperator{\maxf}{max}
\DeclareMathOperator{\hash}{\mathtt{hash}}
\DeclareMathOperator{\rhash}{\mathtt{rhash}}
\DeclareMathOperator{\mhash}{\mathtt{mhash}}
\DeclareMathOperator{\argon}{\mathtt{argon2}}
\DeclareMathOperator{\ciphart}{\mathtt{ciphart}}
\DeclareMathOperator{\cost}{\mathtt{cost}}
\renewcommand{\contentsname}{paper's layout}
\DTMsetdatestyle{iso}
\usepackage{cleveref} % must be loaded last
\begin{document}
\SetAlgorithmName{algorithm}{}{list of algorithms}
\SetInd{.15em}{1em}

\begin{center}
\Huge
ciphart\\
\Large
faster memory-\emph{harder} key derivation \\
with easier security interpretation\\
\normalsize
\emph{caveman}\footnote{mail: \texttt{toraboracaveman [at] protonmail [dot]
com}.}\\
\footnotesize
\DTMnow\\
\end{center}

\noindent\textbf{synopsis---}\emph{argon2}\footnote{\url{https://github.com/P-H-C/phc-winner-argon2}}
is a fast and simple memory-hard key derivation function.  compared to
\emph{scrypt}\footnote{\url{http://www.tarsnap.com/scrypt/scrypt.pdf}},
\emph{argon2} is better, specially for its simplicity.  but i claim that
\emph{argon2} is not fast enough, not memory-hard enough, and its
contribution to our security is not simple enough to understand.

henceforth, i propose \emph{ciphart}, which is:
\begin{itemize}
    \item easier --- because its security contribution is measured in the
    unit of shannon's entropy.  i.e. when \emph{ciphart} derives a key for
    you, it tells you that it has \emph{injected} a specific guaranteed
    quantity of shannon's entropy bits into your derived key.  this is
    possible thanks to my invention, the ``entropy injection'' theorem.

    this offers a great help as it gives us yet another much simpler
    approach to quantify our security gain as opposed to being limited to
    surveying the industry of application-specific integrated circuits as
    done in the \emph{scrypt} paper.

    \item harder --- because it can require crazy-large amounts of memory,
    beyond our random-access memory, thanks to it being able to use the
    hard-disk as well.  this is possible thanks to my discovery ``cacheable
    keys''.

    this is optional, but i extremely like it as it effectively gives me
    much more security while eventually becoming much faster as well, and
    the adversary cannot get my cache even if he steals my hard-disks.

    \item faster --- because it does not abuse hashing functions.  it uses
    hashing functions when using them is more suited, and uses symmetric
    block encryption functions when using them is  more suited.  this is
    thanks to my ``hashing is only for compression'' law.

    \emph{argon2} incorrectly limits itself to only use a hashing function.
    at the surface it may appear simpler, but it is actually more complex
    as it ends up re-inventing what resembles a symmetric block encryption
    function off the hashing function, except for being slower and with
    potential entropy loss.
\end{itemize}

\texttt{libciphart}\footnote{\url{https://github.com/Al-Caveman/libciphart}}
is a library that implements \emph{ciphart} very closely to this paper,
without much fluff.  this should make integrating \emph{ciphart} into other
systems more convenient.

\texttt{ciphart}\footnote{\url{https://github.com/Al-Caveman/ciphart}} is
an application for encrypting and decrypting files that makes use of
\texttt{libciphart}.  this application is intended for use by end-users or
scripts, henceforth it has some fluff to treat mankind with dignity.

\break
\tableofcontents

\section{background}
\subsection{shannon's entropy}
we've got password $p$ with $H(p)$ many shannon's entropy bits worth of
information in it.  so what does this mean\footnote{say that $H(p)$ was
calculated using the base $2$ logarithm, $\log_2$.  all shannon's entropies
in this paper are calculated this way.}?

fundamentally, it means that, on average, we'd need to ask $H(p)$ many
perfect binary questions\footnote{one which, if answered, and on average,
gets the search space reduced in half.} in order to fully resolve all
ambiguities about $p$; i.e.  to fully get every bit of $p$.  

but people use it to do less orthodox things, such as quantifying the
amount of security $p$ has against, say, brute-forcing attacks.

say that we've got a $8V$ bit key $k \gets \hash(p \Vert s, 8V)$, derived
from password $p$, where $s$ is a salt.  say that the attacker has $s$ and
$k$ but wants to figure out $p$.  in this case, he will need to brute-force
the password space in order to find $p$ that gives $k$.  his cost is:
\begin{equation}\label{eq_cost_passbruteforce}
    2^{H(p)} \left(
        \cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}

\begin{definition}
the security of a system is the cost of the cheapest method that can break
it.
\end{definition}

one way to estimate $\cost$ is to survey the asics industry.  by surveying
the asics industry to get an idea how much money it costs to get a given
key, or password, space brute-forced within a target time
frame\footnote{see the \emph{scrypt} paper for an example.}.  this has an
expensive housekeeping and is usually not possible to get any guarantees as
we don't know about state-of-art manufacturing secrets that adversaries may
have.

another way is to ignore anything that has no cryptographic guarantee.  so,
in (\ref{eq_cost_passbruteforce}), cryptography
guarantees\footnote{statistically by confidence earned through peer review
and attempts to break encryption algorithms.} that $2^{H(p)}$ many $\hash$
calls are performed and that many equality tests.  the $\hash$ call needs
to be done once, so let's give it a unit of time $1$.  the equality test
also needs to be called once, but since since it's so cheap it's easier to
just assume that its cost is free.  this way (\ref{eq_cost_passbruteforce})
becomes just:
\begin{equation}\label{eq_simplecost_passbruteforce}
    2^{H(p)} (1+0) = 2^{H(p)}
\end{equation}

further, for convenience, it seems that people report it in the $\log_2$
scale.  i.e.:
\begin{equation}\label{eq_pass_entropy}
    \log_2 2^{H(p)} = H(p)
\end{equation}

i think this is why people use shannon's entropy of passwords as a measure
of their security.  not because it is the quantity of security, but rather
because its the quantity of \emph{simplified} security.

i like using shannon's entropy as a measure of simplified security
quantity, so i'm going to build on it.

\subsection{caveman's entropy}
\subsubsection{recursive $\hash$: $\rhash$}
if the $\hash$ function is replaced by an $N$-deep recursion over $\hash$,
like:
\[
    \begin{split}
        & \rhash(p \Vert s, 8V, N) \\
    ={} &  \hash(\hash(\ldots\hash(p \Vert s, 8V), \ldots, 8V), 8V)
    \end{split}
\]
then, if $\hash$ is not broken,  (\ref{eq_cost_passbruteforce}) becomes:
\begin{equation}\label{eq_cost_passbruteforce_N}
    2^{H(p)} \left(
        N\cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}
(\ref{eq_simplecost_passbruteforce}) becomes:
\begin{equation}\label{eq_simplecost_passbruteforce_N}
    \begin{split}
    2^{H(p)} (N+0) &= N2^{H(p)} \\
                  &= 2^{H(p) + \log_2 N}
    \end{split}
\end{equation}
and the $\log_2$ scaled version becomes:
\begin{equation}\label{eq_pass_cavemanentropy_mhash}
    H(p) + \log_2 N
\end{equation}

obviously (\ref{eq_pass_cavemanentropy_mhash}), which represents the log
scaled version of the simplified security of password $p$ when hashed using
$\mhash$, is no longer equivalent to $p$'s shannon's entropy as was the
case in (\ref{eq_pass_entropy}) when the same password $p$ was hashed by
using just $\hash$.

i think (\ref{eq_pass_cavemanentropy_mhash}) better have a name.  i propose
to call it caveman's entropy, $C$, which i define to be function dependent.
it goes like this:
\begin{equation}
    C\Big(p, \hash(\ldots)\Big) = H(p)
\end{equation}
\begin{equation}
    C\Big(p, \rhash(\ldots, N)\Big) = H(p) + \log_2 N
\end{equation}

\subsubsection{memory-hard $\hash$: $\mhash$}
let $\mhash$ be like $\rhash$, except that it also requires $M$ many memory
bytes such that, as available memory is linearly reduced from $M$, penalty
in cpu time grows exponentially.  let $M$ be requested memory, $A$ be
available memory, and $e(M - A)$ be the exponential penalty value for
reduction in memory, where $e(0) = 1$.
\begin{equation}
    \begin{split}
        & \cost\Big(\mhash(p \Vert s, N, M)\Big) \\
    ={} & \cost\Big(\rhash(p \Vert s, N)\Big)^{e(M-A)}
    \end{split}
\end{equation}

if $\hash$ in (\ref{eq_cost_passbruteforce}) is replaced by the $M$-bytes
memory-hardened $N$-deep recursion hash function $\mhash$, then
(\ref{eq_cost_passbruteforce}) becomes:
\begin{equation}\label{eq_cost_passbruteforce_NM}
    2^{H(p)} \left(
        N^{e(M-A)}\cost(\hash) + \cost(\text{if } \hat k = k)
    \right)
\end{equation}
(\ref{eq_simplecost_passbruteforce}) becomes:
\begin{equation}\label{eq_simplecost_passbruteforce_NM}
    \begin{split}
    2^{H(p)} (N^{e(M-A)}+0) &= N^{e(M-A)} 2^{H(p)} \\
                  &= 2^{H(p) + \log_2 N^{e(M-A)}} \\
                  &= 2^{H(p) + e(M-A)\log_2 N}
    \end{split}
\end{equation}
and caveman's entropy becomes:
\begin{equation}
    C\Big(p, \mhash(\ldots, N, M)\Big) = H(p) + e(M-A)\log_2 N
\end{equation}

\section{fundamental ideas}

\subsection{``entropy injection'' theorem}
\subsubsection{mental warm up}
say that $x$ is something that all ambiguities about it are resolved after
getting $100$ perfect binary questions, $q_0, q_1, \ldots, q_{99}$,
answered.  we say that that's its $\log_2$-based shannon's entropy, right?
i.e. $H(x) = 100$.

what about another thing $y$, which all ambiguities about it are resolved
after getting $10$ questions, $\hat q_0, \hat q_1, \ldots, \hat q_{9}$,
answered, such that each question $\hat q_i$ is made of $10$ perfect binary
questions $q_{i10+0}, q_{i10+1}, \ldots, q_{i10+9}$?  what's the entropy of
$y$?

$H(y) = 100$, because simply bundling $10$ perfect binary questions
together, and calling them $\hat q_i$ won't change the fact that the total
perfect number of perfect binary questions that need to be answered is
still $10 \times 10 = 100$.

so, $H(x) = H(y) = 100$ because question bundling doesn't reduce the
information content.

\begin{lemma}
    bundling perfect binary questions, about a thing, into fewer super
    questions, doesn't reduce thing's information content.
\end{lemma}

\subsubsection{the real deal}

let $p$ be a password with $H(p)$ many shannon's entropy bits.  let $\hat
p$ be a more complex password with $H(p) + e(M-A)\log_2 N$ many shannon's
entropy bits, where $M$, $A$ and $N$ are positive numbers, and $M > A$.

then caveman's entropy says that the following keys are information
theoretically indistinguishable for as long as only $p$ and $\hat p$ remain
unknown\footnote{everything else is known, such as the distribution from
which $p$ and $\hat p$ was randomly sampled.}, and for as long as $\hash$
is not broken:
\begin{itemize}
    \item $k \gets \mhash(p \Vert s, N, M)$
    \item $\hat k \gets \hash(\hat p \Vert s)$
\end{itemize}

in other words:
\begin{equation}
    C\Big(p, \mhash(\ldots, N, M)\Big) = H(\hat p)
\end{equation}

since the assumption that passwords are kept away from the adversary is
fundamental in a symmetric encryption context, i think it makes since that
we measure our security with memory-hard key derivation functions using the
caveman's entropy $C$ instead of shannon's entropy $H$.

from a security point of view, it will feel absolutely identical to as if
the password got injected with extra shannon's entropy bits.  no one can
tell the difference for as long as the fundamental assumption of hiding
passwords is honoured, as well as the hashing function $\hash$ is not
broken.

in other words, we can say, if password $p$ is unknown, and $\hash$ is not
broken, then we have injected into $p$ extra shannon's entropy bits.  this
lie will be only discovered after $p$ is revealed.

if you think that it is impossible for this \emph{lie} to be \emph{truth}
under the secrecy of $p$, then i've done an even better job: proving that
cryptographically secure hashing functions do not exist.  likewise, same
can be trivially extended to: cryptographically symmetric ciphers do not
exist.

so you have to pick only one of these options:
\begin{enumerate}
    \item either accept that the lie is truth.  i.e. accept that we've
    injected shannon's entropy bits into $p$, for as long as only $p$ is
    not revealed.
    \item or, accept that cryptographically-secure hashing and
    symmetric-encryption functions functions do not exist.
\end{enumerate}

\begin{theorem}[entropy injection] \label{theorem_perfect_lie}
    for as long as password $p$ remains a secret and $\hash$ is not broken,
    derived keys $k_1 = \rhash(p, \ldots, N)$ and $k_2 = \mhash(p, \ldots,
    N, M)$ will have their shannon's entropies, $H(k_1)$ and $H(k_2)$,
    increase beyond $H(p)$ in order to equate their caveman's entropies
    $C(p, \rhash(\ldots, N))$ and $C(p, \mhash(\ldots, N, M))$,
    respectively.
\end{theorem}

the reason this lie is appealing is because it simplifies our
quantification of the amount of security that we have gained by using a
given key derivation function, such as $\rhash$ or $\mhash$.

without treating this lie as truth, our only hope would be surveying the
asics industry.  but with this lie, we have one more approach to get a feel
of the gained security quantity by just accepting caveman's entropy $C$ as
shannon's entropy $H$, and move on as if the lie is truth, and no one can
notice it.

we can also look at it from the perspective of \emph{occam's razor}.  i.e.
if two things are not distinguishable from one another, then assuming that
they are just the same thing is simpler than assuming otherwise.  

to be more specific about \emph{occam's razor}: (1) each assumption bit has
a positive probability of error by definition, (2) since assuming that
indistinguishable things are different than one another is more complex
(i.e. more assumption bits) than assuming not, and (3) since there is no
observable difference between the two things, therefore it necessarily
follows that our model's total error will be reduced if we accept that the
indistinguishable things are identical (i.e.  which is what
\cref{theorem_perfect_lie} says).

\subsection{``cacheable keys'' discovery}
\begin{discovery}[cacheable keys]\label{discov_key_caching}
    caching keys securely is easily doable, and great security utility
    exists in doing so for expensively-derived keys.
\end{discovery}

\subsubsection{why does it work}
\begin{itemize}
    \item when expensively derived keys are cached, only the first key
    derivation call will be expensive, while subsequent calls will be
    semi-instantaneous.  this effectively allows users to tolerate much
    more expensive, or secure, key derivation as it only happens during the
    initial, say, login phase.  subsequent use of the extremely expensive
    key is instantaneous.

    so instead of having the user use a somewhat expensive key derivation
    by wayting say, $3$ seconds in each login, he will ---instead--- wait,
    say $10$ seconds in his initial login in order to utilise a much more
    expensive key derivation, and then wait near $0$ seconds for every
    subsequent login as the expensive key is cached.

    \item derived keys can be cached securely, without increasing most
    users' assumptions.  e.g. cached keys can live in a \emph{dm-crypt}
    partition that is encrypted with a large encryption key that is stored
    properly, and the cache can have strict read permissions so that only
    the unique user that runs \texttt{ciphart} executable can read it.

    this only requires to trust the user \texttt{root}, which is already
    trusted by almost everyone.  so we are not introducing a new
    assumption.  
\end{itemize}

for most people, if \texttt{root} is compromised, then the adversary
can break every other key derivation function, including those that do
not cache keys, by simply, say, running a keylogger.

so, practically, we are not increasing the assumptions, but we are only
increasing the value that we can extract from the assumptions that we
already have.

most importantly, utilising discovery \ref{discov_key_caching} allows us to
achieve a memory-\emph{harder} key derivation in an extremely  usable way.
more details on memory-harderness later.

\subsubsection{potential adversary strategies}
let's see what may the adversary try to do against a key caching system:
\begin{itemize}
    \item adversary strategy 1 --- hack into user's system and execute a
    program as that user that tries to read the password cache file to
    obtain the memory-harder key inside it.

    \textbf{answer:} he will not be able to read the file due to strict
    read permissions of the cached files as set earlier.

    \item adversary strategy 2 --- steal user's hard disk and try to mount
    it in his system, to login as root and chance permissions of the key
    cache files.

    \textbf{answer:} the partition where the cached files are saved are
    properly encrypted, so he can't see the cached files, let alone
    changing their read permissions.

    \item adversary strategy 3 --- break into machine's \texttt{root}
    account.

    \textbf{answer:} he will succeed, but then he can also run a keylogger,
    which will also break every other key derivation function, even those
    who do not use key caching, such as \emph{scrypt}, \emph{argon2}, etc.
\end{itemize}

\subsection{``hashing is only for compression'' law}
which one is simpler when, say, building a wooden house?  
\begin{itemize}
    \item option 1 --- use only nails and, when you need screws,
    modify some nails into screws.
    \item option 2 --- use nails and screws.
\end{itemize}

on the surface, option 1 may appear as the simpler choice as it only uses
nails, while option 2 uses both nails ans crews.

but a deeper look shows that option 1 is actually a lie, as it is also
using screws alongside nails, except that the screws are constrained by
being re-invented by modifying nails.  in other words, option 1 has the
extra assumption that its screws must be made using nails, while option 2
does not have this extra assumption.  hence option 2 is actually simpler.

my answer with \emph{ciphart} is that option 2 is the simpler choice
because it removes the re-invention aspect, specially if the re-invented
screws were worse than screws that were made as screws from the start.

on the other hand, when \emph{argon2} acts as if option 1 is better, which
is wrong at every level.

before i write about how \emph{argon2} is an example of adopting the
mistake in option 1, i want to define the nails and the screws of a key
derivation function, strictly from the perspective of key derivation
functions.

\begin{definition}[hashing functions as seen by key derivation
functions]\label{def_kdf_hash}
    a function that maps input to output such that:
    \begin{itemize}
        \item unlimited input --- input is an unbounded number of
        concatenated data chunks.
        \item compression --- input's shannon's entropy bits could be
        larger than the total bit size of the output.
        \item preserving entropy is tried --- output should have as much
        entropy bits from the input, but entropy loss is possible.
        \item walking backwards is extremely hard --- analysing the output
        to find the input is computationally too hard.
    \end{itemize}
\end{definition}

\begin{definition}[symmetric block encryption functions as seen by key
derivation functions]\label{def_kdf_enc}
    a function that maps input to output such that:
    \begin{itemize}
        \item limited input --- fixed sized data and a fixed sized
        key.
        \item preserving entropy is guaranteed --- no entropy loss is
        possible.  the proof is that, if the encryption key is known, we
        can bring back every input bit from the output by decryption.
        \item walking backwards is extremely hard --- analysing the output
        to find the input is computationally too hard.
    \end{itemize}
\end{definition}

when \emph{argon2} completes solving the tasks in its memory pad, it
derives the output key by hashing certain chunks of bytes in the pad.  the
number of hashed chunks depends on pad's size, so it's not of a fixed size,
and henceforth meets the properties of a hashing function shown in
\cref{def_kdf_hash}.  therefore, \emph{argon2} using a hashing function at
this stage is justified.

when \emph{argon2} is solving tasks in the memory pad, it still uses a
hashing function, despite the fact that it is strictly dealing with inputs
of fixed lengths, which also meets the properties of a symmetric block
encryption function in \cref{def_kdf_enc}.  here, \emph{argon2} better use
a symmetric block encryption function instead of a hashing function.  using
a hashing function here is a problem due to:
\begin{itemize}
    \item re-invention of wheels --- having \emph{argon2} concatenate two
    inputs of a fixed size in order to derive an output of the same size is
    effectively an attempt to re-invent a symmetric block encryption
    function.  i.e. the concatenation is used to emulate the effect of
    having a pre-shared key which exists in symmetric block encryption
    functions.  why re-invent keys by concatenation when there exists
    functions that already have keys?

    \item needless risk of entropy loss --- using a hashing function when
    dealing with fixed input sizes needlessly increases the probability of
    having potential entropy losses.  this is due to the fact that hashing
    functions \emph{try} to preserve input's entropy, but cannot guarantee
    it, while symmetric block ciphers \emph{do} guarantee it.  so why have
    the possibility of losing entropy bits when you don't have to?

    \item slower memory filling rate --- generally speaking, hashing
    functions tend to be slower than symmetric block encryption functions.
    this is because of dealing with compression is harder than not.

    symmetric block ciphers guarantee preserving input's entropy in the
    output with much less effort thanks to the fact that the output is at
    least as large as the input.  

    but hashing functions don't have the luxury of having an output that's
    as large as the input, thus they need to work a lot more in order to
    ensure that no input entropy is needlessly lost.

    this slowness is bad as it reduces number of passes over the memory pad in
    a unit of time.  more passes over the memory pad are important for
    strengthening the memory hardness.
\end{itemize}

\begin{law}[hashing is only for compression]\label{law_simplification}
    use hashing functions only when compression happens.  otherwise, use
    symmetric block encryption functions.
\end{law}

\section{ciphart}
\subsection{the algorithm}
\subsubsection{parameters}
\begin{tabularx}{\columnwidth}{lX}
    $P$ & password.\\
    $S$ & salt.\\
    $M$ & total random-access memory in bytes.\\
    $D$ & total hard-disk memory in bytes.\\
    $F$ & temporary file's path.\\
    $Y$ & whether key caching is enabled.\\
    $L$ & number of memory lanes for concurrency.\\
    $T$ & number of tasks per lane segment.\\
    $H$ & number of lane segments per hard-disk read.\\
    $B$ & minimum \emph{caveman's entropy bits} to inject into $p$.\\
    $K$ & output key's size in bytes.\\
\end{tabularx}

\subsubsection{internal variables}
\begin{tabularx}{\columnwidth}{lX}
    $\enc$      & encryption function.\\
    $\hash$     & hashing function.\\
    $\fread$    & hard-disk file reading function, with seeking.  e.g.
                    $\fread(x, y, z)$ reads $z$ many bytes from file $x$
                    after seeking $y$ bytes forward.\\
    $\fwrite$   & hard-disk file writing function.\\
    $C$         & $\gets \begin{cases}
                        64 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is \emph{aes}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this to reflect the block size of the encryption
                    algorithm that implements $\enc$.\\
    $V$ & $\gets \begin{cases}
                        32 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-128}}\\
                        32 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-256}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this is the size of the encryption key that's used to
                    solve \emph{ciphart}'s tasks.  this is different than
                    output key's size, $K$, which is $\enc$-independent.\\
    $\hat T$    & $\gets \maxf(\lceil V C^{-1}\rceil, T)$.  this
                    is to ensure that we have enough encrypted bytes for
                    new keys.\\
\end{tabularx}
\begin{tabularx}{\columnwidth}{lX}
    $\hat T$    & $\gets \hat T - (\hat T \bmod 2) + 2$.  this is to ensure
                    that there is an even number of tasks in a segment.
                    why?  because we need a buffer for storing the
                    clear-text and another for storing the output
                    cipher-text.\\
    $\hat M$    & $\gets M - (M \bmod C\hat TL) + C\hat TL$.  this is to
                    ensure that it is in multiples of $C\hat TL$.  why?  so
                    that all segments are of equal lengths in order to
                    simplify \emph{ciphart}'s logic.  e.g. it wouldn't be
                    nice if the last segments were of unequal sizes.\\
    $G$         & $\gets \hat MC^{-1}\hat T^{-1}L^{-1}$.  total number of
                    segments per lane.\\
    $n$    & $\gets 0$.  actual number of times $\enc$ is called.\\
    $m_i$       & $C$-bytes memory for $i^{th}$ task in the $\hat M$-bytes
                    pad.\\
    $n_l$       & $\gets \maxf(\text{nonce})L^{-1}l$.  nonce variable for $l^{th}$ lane.\\
    $f$         & $\gets 0$.  a counter indicating number of times
                    memory is filled with $\hat M$ many bytes.\\
    $d$         & $\gets 0$.  a counter indicating total number of saved
                    blocks into hard-disk.\\
    $h$         & $\gets 0$.  a counter indicating number of processed lane
                    segments since the last hard-disk read.\\
    $u$         & $\gets 0$.  a counter indicating number of times key was
                    updated from the hard-disk.\\
    $*v$         & $\gets *\hash(P \Vert S \Vert M \Vert D \Vert \ldots
                    \Vert K, V)$.  a pointer to the first byte where
                    $V$-bytes key is stored.  $v$ is the key itself, and
                    $*v$ is a pointer to it.\\
    $Z$         & $\gets \hash(v \Vert 0, V)$.  file name where output key,
                    $k$, is expected to be cached, if $k$ was previously
                    cached.\\
\end{tabularx}

\subsubsection{output}
\begin{tabularx}{\columnwidth}{lX}
$k$ & $K$-bytes key.\\
$n$ & total number of times $\enc$ was actually called.  $\log_2 n$ is
        total number shannon's entropy bits that \emph{ciphart} injected
        into $k$, such that $\log_2 n \ge B$.\\
$\hat M$ & actual number of bytes required to exist in random-access
            memory.\\
$d$     & actual number of bytes required to exist in hard-disk.\\
\end{tabularx}

\subsubsection{steps}
steps of \emph{ciphart} is shown in \cref{alg_ciphart}.  this corresponds
to \emph{argon2d}.  adding a \emph{ciphart-i} variant is a trivial matter,
i just didn't do it yet because my threat model currently doesn't benefit
from a password independent variant.

\begin{algorithm}
\If{$Y$ {\bf and} $\fexists(Z)$}{\label{ciphart_cache_hit}
    $k, n, d \gets \fread(Z, 0, V + \sizeof(k\Vert n\Vert d))$\;
    \Return{$k, n, d$}
}
\While{$1$}{
    \For{$g \gets 0, 1, \ldots, G-1$}{
        \For{$l \gets 0, 1, \ldots, L-1$}{\label{ciphart_lanes}
            \For{$t \gets 0, 1, \ldots, T-1$}{
                $i \gets gLT + lT + t$\;
                \uIf{$t < T - 1$}{
                    $j \gets i + 1$\;
                }\ElseIf{$t = T - 1$}{
                    $j \gets i - T + 1$\;
                }
                $m_j \gets \enc(m_i, n_l, *v)$\;
                $n_l \gets n_l + 1$\;
                \uIf{$f = 0$}{
                    $*v \gets m_j \bmod (gLTC + tC - V)$\;
                    \If{$*v \ge gLTC - V$}{
                        $*v \gets *v + lTC$\;
                    }
                }\Else{
                    $*v \gets m_j \bmod (\hat M - LTC + tC - V)$\;
                    \uIf{$*v \ge gLTC + tC - V$}{
                        $*v \gets *v + LTC$\;
                    }\ElseIf{$*v \ge gLTC - V$}{
                        $*v \gets *v + lTC$\;
                    }
                }
            }
        }
        $n \gets n + LT$\;
        \uIf{$d \le D$}{\label{ciphart_D_filling}
            \For{$i \gets gLT, \ldots, gLT+(T-1)$}{
                $\fwrite(F, m_i)$\;
                $d \gets d + 1$\;
                \If{$d \ge D$}{ \textbf{break}\; }
            }
        }\Else{
            $h \gets h + L$\;
            \If{$h \ge H$}{\label{ciphart_D_using}
                $*v \gets *\fread(F, v \bmod (d - V), V)$\;
                $u \gets u + 1$\;
                $h \gets 0$\;
            }
            \If{$f \ge 1$ \bf and $u \ge 1$ \bf and $n \ge 2^B$}{
                $g_{\text{last}} \gets g$\;
                \textbf{go to} \cref{ciphart_out}\;
            }
        }
    }
    $f \gets f+1$\;
}
$i \gets g_{\text{last}}LT$\;\label{ciphart_out}
$k \gets \hash(m_{i+0T} \Vert m_{i+1T} \Vert \ldots \Vert m_{i+(L-1)T}, K)$\;
\If{$Y$}{\label{ciphart_cache_miss}
    $\fwrite(Z, k \Vert n \Vert d)$\;
}
$\delete(F)$\;
\Return{$k, n, d$}
\caption{ciphart}
\label{alg_ciphart}
\end{algorithm}

\subsection{noteworthy features}
\subsubsection{parallelism}
since iterations of the loop in \cref{ciphart_lanes} in \cref{alg_ciphart}
are fully independent of one other, they can quite happily utilise $L$ cpu
cores, specially when segment sizes, $T$, are larger.

\subsubsection{memory-hardness}
\begin{proof}
    \cref{alg_ciphart} is just a variation of \emph{argon2d}.  so if
    \emph{argon2d} is memory-hard, then so is \emph{ciphart}.
\end{proof}


\subsubsection{memory-\emph{harder}ness}
thanks to discovery \ref{discov_key_caching}, we can cache keys, as done in
\cref{ciphart_cache_hit}, without increasing assumptions of the threat
model of the vast majority of users.  then, memory-\emph{harder}ness
becomes possible.  this process goes like this:
\begin{enumerate}
    \item starts by running an $\enc$-based variant of \emph{argon2},
    except that, as it is going, it keeps writing the updated segments into
    the hard-disk until the size of it satisfies the $D$ bytes limit.  this
    is shown in \cref{ciphart_D_filling} in \cref{alg_ciphart}.

    optimising this hard-disk filling with $D$ bytes is not a big deal,
    since this feature is probably going to be used only once; thanks to
    key caching.  that said: 
    \begin{itemize}
        \item this feature is optional.  i.e. in case someone doesn't like
        the hard-disk caching, he can set $D \gets 0$ to disable it.  but,
        for most people, i don't understand why you would want to disable
        it.  e.g. if you're already trusting \texttt{root}, then i think
        that you can use this feature without changing your threat model.

        i personally like it a lot as it allows me to achieve
        memory-\emph{harder}ness way beyond my random-access memory.  just
        imagine the look on the face of those asics crackers once they hear
        that your \emph{ciphart} requires, say, $50$ giga bytes!

        \item when key caching is enabled, i.e. $Y \gets 1$, this hard-disk
        writing is done only initially, and subsequent uses appear as
        almost instantaneous.

        \item this hard-disk writing can be slightly optimised by using a
        non-blocking write operation.  but i think that the trivial
        reduction in this time may not justify increasing  code's
        complexity, so i don't plan to implement non-blocking writes in
        \texttt{libciphart}.
    \end{itemize}

    \item then, once the $D$ bytes hard-disk requirement is satisfied, the
    process continues to run the modified $\enc$-based variant of
    \emph{argon2} except that it updates the key $v$ from those $D$ bytes
    every time $S$ many segments are solved.  this step makes
    \emph{ciphart} require $D+M$ bytes.  this is shown in
    \cref{ciphart_D_using} in \cref{alg_ciphart}.

    if $S$ is large enough, this can be done efficiently without blocking
    the cpu noticeably, as the randomly obtained $V$ bytes can be read by
    using the $O(1)$ operation, \texttt{seek}, over the $D$ bytes.

    \item delete the $D$ bytes, just to free up the disk space.  no need to
    securely delete those $D$ bytes since we can save them using a
    temporary random key that's forgotten later on.
\end{enumerate}


\subsection{comparison}

\section{application scenarios}
\subsection{a currently-useful scenario}
user has a password manager which generates unique $256$ bit entropy keys for
each online service that he uses.  the user also renews keys for his online
services every now and then.   so his online accounts generally have high
security.  

but user's problem is how to lock and unlock his password manager's
passwords database.  should he use a physical usb-stick key that types a
high-entropy key that encrypts and decrypts the database?   the user
doesn't want this physical key because of several reasons:
\begin{itemize}
    \item he tends to lose his keys a lot, and, for certain tasks, the risk
    of needing to wait for until he gets a backup usb-stick key is too
    much.

    \item he doesn't want to be caught having cryptographic usb-stick keys,
    because as such is an evidence that he has encrypted content.  the user
    wants to have the choice to lie that he has no clue.  so not having
    usb-stick keys with him helps his case to lie.

    \item he wants to be torture-resistant so that an adversary cannot
    forcefully take his keys from him in order to login into his services.
    he may rather want to die than to give the password to the adversary.
    this works because, so far, the brain is a pretty private information
    store.
\end{itemize}

in this scenario, the user memorises a sensible password that he can
remember, with enough initial entropy, and then uses \emph{ciphart},
preferably with disk caching, to inject large amounts of entropy bits into
his derived keys, way beyond the reach of pre-\emph{ciphart} key derivation
functions; thanks to \cref{theorem_perfect_lie} and discovery
\ref{discov_key_caching}.  

here, the expensively derived key is only used to unlock a local password
manager, which offers a protection against situations where a backup copy
of the passwords database is stolen.  this may enable the user to store his
passwords manager in an online file synchronisation service for more
convenient system migrations to further reduce his login delays should he
face the need to migrate to a new, say, personal computer.

\begin{advertisement}
    in case you're interested in such a passwords manager, i've also made
    \emph{nsapass}\footnote{\url{https://github.com/Al-Caveman/nsapass}}
    --- a flexible and a simple passwords manager in a few hundreds of
    python lines of code, that uses \emph{ciphart} by default.  i think
    this is the best command-line interfacing passwords manager by far, for
    its usability, and for the fact that auditing it is easy, thanks for it
    being only in a few hundreds of python lines.
\end{advertisement}

\subsection{a later-useful scenario}
all input password fields will internally call \texttt{libciphart} to
derive more expensive keys.  this way, applications, such as
\emph{firefox}, \emph{mutt}, \ldots, will never send actual passwords, but
will only send \emph{ciphart}-derived keys with increased shannon's entropy
content.

thanks to \cref{theorem_perfect_lie}, this will automatically increase
shannon's entropy of all users' passwords without requiring users to
memorise harder passwords.  thanks to discovery \ref{discov_key_caching},
the user will not face any delay in his daily use, except only an initial
delay to create the cache entry.

i also think that it is \emph{generally} better to have expensive key
derivation functions in the client side as opposed to the server side.
because remote servers always have the incentive to reduce the complexity
of the key derivation function in order to free more resources for other
things that bring them money.

either  way, \emph{ciphart} is perfectly usable on the server side.  it is
just that i think \emph{ciphart}, \emph{argon2}, \emph{scrypt}, \ldots make
better sense when placed on the client side.

\vfill
\break
\appendix
\section{donations}
this work is sponsored by an unexpected nerdiness.  nothing here was
supposed to happen.  the cause  remains unknown, so we call it
\emph{something random with a large entropy}.

ancient mythology has it that, every time a donation is made for a good
cause, a beautiful torch is kindled somewhere deep within an otherwise cold
and dark space.

the torch is pure with an innocent smile every time a spending is committed
towards good.  but, she drops a tear when the spending is done in excess.
despite her attempts to maintain a steady light, sometimes her tears force
her light to flicker, and sometimes the flickering causes her to disappear.
when she is gone, she is never back again, except in the memories of those
who have once witnessed her charm\ldots

so, if you opt for a donation, i'd appreciate exercising wisdom, so that
the sensible balance is not exceeded; lest we want a beautiful torch to be
born, only to make her disappear in her tears.

\subsection{bitcoin}
\begin{center}
    \includegraphics[width=121px]{./pics/btc_wallet_address_trimmed.png}
    \texttt{bc1qtylzjtgd0yu4v7f8hyfzufn7nu692v9fc88jln}
\end{center}

\end{document}
