\documentclass[twocolumn]{article}
\usepackage[margin=.7in]{geometry}
\usepackage[showisoZ=false]{datetime2}
\usepackage{url}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\DeclareMathOperator{\enc}{enc}
\DeclareMathOperator{\maxf}{max}
\DeclareMathOperator{\len}{len}
\DeclareMathOperator{\newkey}{newkey}
\renewcommand{\contentsname}{content}
\makeatletter
\def\myrulefill{%
    \leavevmode\leaders\hrule%
    height .6ex width 1ex depth -0.4ex%
    \hfill\kern\z@%
}
\makeatother
\DTMsetdatestyle{iso}
\usepackage{cleveref} % must be loaded last
\begin{document}
\SetAlgorithmName{algorithm}{}{list of algorithms}
\SetInd{.1em}{1em}

\begin{center}
\Huge
\myrulefill\ ciphart \myrulefill\\
\LARGE
memory-hard key derivation \\
with easier measurable security\\
\normalsize
caveman\footnote{toraboracaveman [at] protonmail [dot] com}\\
\footnotesize
\DTMnow\\
\rule{1\columnwidth}{2pt}
\end{center}

\emph{argon2}\footnote{\url{https://github.com/P-H-C/phc-winner-argon2}} is
mostly nice, but trying to interpret its contribution to the protection
against password brute-forcing attacks remains more difficult than it
should be.  this vagueness is a problem that is not limited to
\emph{argon2}, but also shared with every other key derivation function
that i've known so far.

when one uses \emph{argon2}, his derived key will surely have superior
protection against password brute-forcing attacks, but by how much?  to
answer this, one would need to survey the industry that manufactures
application-specific integrated circuits (asics) to obtain a map between
\emph{time} and \emph{money}, in order to get an estimation on how much
would it cost the adversary to discover the password in a given time
window.

while the approach of surveying the asics industry is not wrong, it is
largely subjective, with expensive housekeeping, and practically leads the
user to rely on vague foundations to build his security on.  this vagueness
is not nice, and it would be better if we had an objective measure to
quantify the security of our memory-hard key derivation functions.

resolving this vagueness is not a mere luxury to have, but a necessity for
maximising survival, because it hinders the process of studying the
cost-value of memory-hard key derivation functions, which, effectively,
increases the risk of having a false sense of security.

so i propose \emph{ciphart} --- a memory-hard key derivation function with
a security contribution that is measured in a unit that i call
\emph{relative entropy bits}.  this unit is measured objectively and is
guaranteed to be true irrespective of whatever alien technology that the
adversary might have.

\texttt{libciphart}\footnote{\url{https://github.com/Al-Caveman/libciphart}}
is a library that implements \emph{ciphart} very closely to this paper,
without much fluff.  this should make integrating \emph{ciphart} into other
systems more convenient.

\texttt{ciphart}\footnote{\url{https://github.com/Al-Caveman/ciphart}} is
an application for encrypting and decrypting files that makes use of
\texttt{libciphart}.  this application is intended for use by end-users or
scripts, henceforth it has some fluff to treat mankind with dignity.
\vfill
\break

\tableofcontents
%\vfill
%\break

\section{ciphart}
\subsection{parameters}
\begin{tabularx}{\columnwidth}{lX}
    $\enc$ & encryption function.\\
    $p$ & password.\\
    $s$ & salt.\\
    $M$ & total memory in bytes.\\
    $L$ & number of memory lanes for concurrency.\\
    $T$ & number of tasks per lane segment.\\
    $R$ & number of rounds per task.\\
    $B$ & minimum quantity of increased protection against password
            brute-forcing attacks in the unit of \emph{relative entropy
            bits}.\\
    $K_{\text{out}}$ & output key size in bytes.\\
\end{tabularx}

\subsection{internal variables}
\begin{tabularx}{\columnwidth}{lX}
    $C$         & $\gets \begin{cases}
                        64 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is \emph{aes}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this to reflect the block size of the encryption
                    algorithm that implements $\enc$.\\
    $K_{\text{in}}$ & $\gets \begin{cases}
                        32 \text{ bytes} & \text{if $\enc$ is
                                            \emph{xchacha20}}\\
                        16 \text{ bytes} & \text{if $\enc$ is
                            \emph{aes-128}}\\
                        \ldots & \\
                     \end{cases}$\\
                & this is the size of the encryption key that's used to
                    solve \emph{ciphart}'s tasks.  this is different than
                    the $\enc$-independent $K_{\text{out}}$ which is
                    possibly used by other encryption algorithms in later
                    stages\footnote{at the expense of losing the meaning of
                    \emph{relative entropy bits}.}.\\
    $\hat T$    & $\gets \maxf(\lceil K_{\text{in}} / C\rceil, T)$.  this
                    is to ensure that we have enough encrypted bytes for
                    new keys.\\
    $\hat T$    & $\gets T - (T \bmod 2) + 2$.  this is to ensure that
                    there is an even number of tasks in a segment.  why?
                    because we need a buffer for storing the clear-text and
                    another for storing the output cipher-text.\\
\end{tabularx}
\begin{tabularx}{\columnwidth}{lX}
    $\hat M$    & $\gets M - (M \bmod C\hat TL) + C\hat TL$.  this is to
                    ensure that it is in multiples of $C\hat TL$.  why?  so
                    that all segments are of equal lengths in order to
                    simplify \emph{ciphart}'s logic.  e.g. it wouldn't be
                    nice if the last segments were of unequal sizes.\\
    $G$         & $\gets \hat MC^{-1}\hat T^{-1}L^{-1}$.  total number of
                    segments per lane.\\
    $\hat B$    & $\gets \maxf(\hat MC^{-1}R, 2^B)$.  this is to ensure
                    that $\hat B$ is large enough to have at least one pass
                    over the $\hat M$-bytes memory.\\
    $\hat B$    & $\gets \hat B - (\hat B \bmod L\hat TR) +
                    L\hat TR$.  this is to reflect the reality that, with
                    \emph{ciphart}, segments must complete.  i.e. when the
                    user asks for $B$ \emph{relative entropy bits}, he gets
                    $\log_2 \hat B$ bits instead, where $\log_2\hat B \ge
                    B$.\\
    $m_i$       & $C$-bytes memory for $i^{th}$ task in the $\hat M$-bytes
                    pad.\\
    $n_l$       & $\gets lG\hat TR$.  nonce variable for $l^{th}$ lane with
                    at least $64$ bits.\\
    $f$         & $\gets 0$.  a flag indicating whether the $\hat M$-bytes
                    pad is filled.\\
\end{tabularx}

\subsection{output}
\begin{tabular}{ll}
$k$ & $K_{\text{out}}$-bytes key with $\ge B$ \emph{relative entropy
        bits}.\\
\end{tabular}

\subsection{steps}
steps of \emph{ciphart} is shown in \cref{alg_ciphart}.  this corresponds
to \emph{argon2d}.  adding a \emph{ciphart-i} variant is a trivial matter,
i just didn't do it yet because my threat model currently doesn't benefit
from a password independent variant.

\begin{algorithm}[!h]
\While{$1$}{
    \leIf{$f = 0$}{$\hat R \gets 1$}{$\hat R \gets R$}
    \For{$g=0, 1, \ldots, G-1$}{
        \For{$l=0, 1, \ldots, L-1$}{\label{ciphart_lanes}
            \For{$t=0, 1, \ldots, T-1$}{
                $i \gets gLT + t$\;
                \uIf{$t < T - 1$}{
                    $j \gets i + 1$\;
                }\ElseIf{$t = T - 1$}{
                    $j \gets i - T + 1$\;
                }
                \For{$r=0, 1, \ldots, \hat R-1$}{
                    $m_j \gets \enc(m_i, n_l, k)$\;
                    $\hat i \gets i$\;
                    $i \gets j$\;
                    $j \gets \hat i$\;
                    $n_l \gets n_l + 1$\;
                    \uIf{$f = 0$}{
                        $v \gets m_j \bmod (gLT + t)$\;
                        \If{$v \ge gLT$}{
                            $v \gets v + lT$\;
                        }
                    }\Else{
                        $v \gets m_j \bmod (GLT - LT + t)$\;
                        \uIf{$v \ge gLT + t$}{
                            $v \gets v + LT$\;
                        }\ElseIf{$v \ge gLT$}{
                            $v \gets v + lT$\;
                        }
                    }
                    $k \gets m_v[0:K_{\text{in}}]$\;\label{ciphart_newkey}
                }
            }
        }
        \If{$n_1L = \hat B$}{
            $g_{\text{last}} = g$\;
            \textbf{go to} \cref{ciphart_out}\;
        }
    }
    $f \gets 1$\;
}
\While{$1$}{\label{ciphart_out}
    \For{$l=1, 2, \ldots, L$}{
        \lIf{$\len(k) \ge K_{\text{out}}$}{\Return $k[0:K_{\text{out}}]$}
        $n \gets n + 1$\;
        $k \gets k \mathbin\Vert \enc(m_{l,S,T}[1], n, k)$\;
    }
}
\caption{ciphart}
\label{alg_ciphart}
\end{algorithm}

\section{parallelism}
since iterations of the loop in \cref{ciphart_lanes} in \cref{alg_ciphart}
are fully independent of one other, they can quite happily utilise $L$ cpu
cores, specially when segment sizes, $T$, are larger.

other lines are not easily independent, so i didn't even bother to try to
parallelise them.  specially since this is not a problem, since the
cpu-heavy part is in the easily-parallelise-able  part.

with \emph{argon2}, if one wants to increase the cpu load without
increasing memory pad's use, one can increase the number of passes over the
pad.  this feature is supported by \emph{ciphart} via the \emph{relative
entropy bits} parameter $B$.

but, simply increasing number of passes on the pad may not be the best
option for all cases.  e.g. what if someone has a small pad, and small
segments?  in such case, a higher percentage of the cpu will be wasted in
the non-parallelise-able steps, which is a waste.

this is why \emph{ciphart} has an additional parameter $R$.  this parameter
can allow to increase the load on the cpu without even requiring to go
through the non-parallelise-able steps.  \emph{argon2} lacks this
parameter.  this is not the main reason \emph{ciphart} was made, but it's
one of the incremental improvements.

philosophically, i think that \emph{argon2} does have the $R$ parameter,
except that it assumes that $R=1$, and does not allow the user to change
it.  i don't agree with this assumption, which is why i made \emph{ciphart}
to allow the user to set $R$ more freely.

however, i think that $R=1$ is best for the first pass to ensure that the
memory is filled soonest possible.  this is why \emph{ciphart} assumes
$R=1$ in the first pass, just to fillt he memory as soon as possible.  but
in subsequet passes, user's value for $R$ will be chosen.  i.e. if $R=5$,
then each task in later passes will be solved recursively $5$ times.

\section{memory-hardness}
\section{security interpretation}
\section{comparison}
\section{summary}

\end{document}
